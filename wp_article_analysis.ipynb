{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10768d73-ba5e-47a1-8c4c-f303d52568b6",
   "metadata": {},
   "source": [
    "# Considering Bias in Data\n",
    "Author: Amit Peled\n",
    "\n",
    "Homework 2\n",
    "\n",
    "## Requesting ORES scores through LiftWing ML Service API\n",
    "\n",
    "Wikimedia Foundation (WMF) is reworking access to their APIs. It is likely in the coming years that all API access will require some kind of authentication, either through a simple key/token or through some version of OAuth. For now this is still a work in progress. You can follow the progress from their [API portal](https://api.wikimedia.org/wiki/Main_Page). Another on-going change is better control over API services in situations where those services require additional computational resources, beyond simply serving the text of a web page (i.e., the text of an article). Services like ORES that require running an ML model over the text of an article page is an example of a compute intensive API service.\n",
    "\n",
    "Wikimedia is implementing a new Machine Learning (ML) service infrastructure that they call [LiftWing](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing). Given that ORES already has several ML models that have been well used, ORES is the first set of APIs that are being moved to LiftWing.\n",
    "\n",
    "This example illustrates how to generate article quality estimates for article revisions using the LiftWing version of [ORES](https://www.mediawiki.org/wiki/ORES). The [ORES API documentation](https://ores.wikimedia.org) can be accessed from the main ORES page. The [ORES LiftWing documentation](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing/Usage) is very thin ... even thinner than the standard ORES documentation. Further, it is clear that some parameters have been renamed (e.g., \"revid\" in the old ORES API is now \"rev_id\" in the LiftWing ORES API).\n",
    "\n",
    "## Article Page Info MediaWiki API \n",
    "This notebook also illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). It requests summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024\n",
    "\n",
    "Note: This project was developed with the assistance of ChatGPT, which helped format code and organize the data and analysis in an efficient and clear manner. All content has been verified and tested by the author.\r",
    "d\n",
    "\n",
    "This notebook demonstrates how to use the LiftWing API to generate article quality estimates for Wikipedia article revisions using the ORES model.\n",
    "\n",
    "* ORES API Documentation: [ORES API documentation](https://ores.wikimedia.org)\n",
    "* LiftWing Documentation: [LiftWing ORES API Documentation](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing/Usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de19aa-cbcd-40bb-b9ad-8d258e351a5d",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "In this notebook, I aim to analyze the coverage and quality of Wikipedia articles about political figures across various countries. I will:\n",
    "\n",
    "- Retrieve article metadata and ORES quality predictions using the Wikimedia API and the LiftWing infrastructure.\n",
    "- Merge this data with population statistics for each country and geographic region.\n",
    "- Calculate per capita coverage for both articles and high-quality articles.\n",
    "- Analyze geographic patterns by producing tables that rank countries and regions by article coverage and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad8c6d-97c9-4bc4-9b7b-972ecf0b3645",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4879999-8f24-4218-b7b8-cb5a3cef8d8b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7ab3468e-0de0-4324-b902-f8a049cc9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse, pandas as pd\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535fc4a-c319-486f-a2a2-1e9ef954d275",
   "metadata": {},
   "source": [
    "### Load CSV Data (Politicians and Population) \n",
    "These files are located in the repositiry of this project, and were provided to me by the teaching staff of DATA 512 for the purposes of this assingment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3320194d-5242-49ff-8c8c-78ce1d51aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country  \n",
      "0  Afghanistan  \n",
      "1  Afghanistan  \n",
      "2  Afghanistan  \n",
      "3  Afghanistan  \n",
      "4  Afghanistan  \n",
      "         Geography  Population\n",
      "0            WORLD      8009.0\n",
      "1           AFRICA      1453.0\n",
      "2  NORTHERN AFRICA       256.0\n",
      "3          Algeria        46.8\n",
      "4            Egypt       105.2\n"
     ]
    }
   ],
   "source": [
    "# Load politicians data\n",
    "politicians_df = pd.read_csv('data/politicians_by_country_AUG.2024.csv')\n",
    "\n",
    "# Load population data\n",
    "population_df = pd.read_csv('data/population_by_country_AUG.2024.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe\n",
    "print(politicians_df.head())\n",
    "print(population_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df06c94-9172-4fc1-a180-27c772a7b6cc",
   "metadata": {},
   "source": [
    "### Data Cleaning and Handling Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f0702615-79bf-4c2e-8241-e30d4e8e6701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in Politicians Data: 0\n",
      "Missing values in Politicians Data: name       0\n",
      "url        0\n",
      "country    0\n",
      "dtype: int64\n",
      "          Geography  Population\n",
      "0             WORLD      8009.0\n",
      "1            AFRICA      1453.0\n",
      "2   NORTHERN AFRICA       256.0\n",
      "10   WESTERN AFRICA       442.0\n",
      "27   EASTERN AFRICA       483.0\n",
      "  Geography  Population\n",
      "3   Algeria        46.8\n",
      "4     Egypt       105.2\n",
      "5     Libya         6.9\n",
      "6   Morocco        37.0\n",
      "7     Sudan        48.1\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in politicians data\n",
    "print(\"Duplicates in Politicians Data:\", politicians_df.duplicated().sum())\n",
    "\n",
    "# Check for missing values in politicians data\n",
    "print(\"Missing values in Politicians Data:\", politicians_df.isnull().sum())\n",
    "\n",
    "# Split population data into regions and countries\n",
    "regions_df = population_df[population_df['Geography'].str.isupper()]\n",
    "countries_df = population_df[~population_df['Geography'].str.isupper()]\n",
    "\n",
    "# Check the first few rows to ensure correct splitting\n",
    "print(regions_df.head())\n",
    "print(countries_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05aca0-886f-477f-90ba-53998e8d0c85",
   "metadata": {},
   "source": [
    "## Making API Requests for Article Metadata and ORES Quality Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb6b33-64d3-4d44-a8aa-8b6f376fbca4",
   "metadata": {},
   "source": [
    "### Constants and Setup for API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "369355f7-4137-4fd8-b9c2-7c39dfd694f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "ORES_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/enwiki\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"apeled@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<apeled@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d6544-f214-46df-a9ce-22ed379bbf2b",
   "metadata": {},
   "source": [
    "### Function to Get Wikipedia Article Metadata (Page Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "244f2ed1-2652-4abe-a7e4-63c81d6dd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e46e5-750c-4d64-bbe2-3915199aa4b5",
   "metadata": {},
   "source": [
    "### Function to Get ORES Article Quality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "361a5cd0-21a9-4caf-9a49-85bc99c7cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aebe7b-2073-4bdd-af54-c51ebc4f6f36",
   "metadata": {},
   "source": [
    "## Running Requests for Each Article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bb378-31a8-4b1e-80f1-cbd79f9f2078",
   "metadata": {},
   "source": [
    "### Retrieve Revision IDs for Each Article and then the ORES scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056475b8-3e3a-46b0-bbbd-1678b76d16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your actual email and ORES API access token\n",
    "email_address = \"apgsw30@gmail.com\"\n",
    "access_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiJjM2UzMTkyZGE4YWNmNDc5ZTgzODA5MDkwZWY0NTBjMCIsImp0aSI6ImIwY2NlMGM3ZGQwNGRlYmQ0Yjc1ZDc4ZjRiOGM1ZmU2ZjFiZWQ5MmJiNTc4NmNjYTQxM2I3MTI0ZTIwNjBhMDcyNjBiNDViMGZkYTA3NTUyIiwiaWF0IjoxNzI4ODU3MjMyLjM2NTEzNywibmJmIjoxNzI4ODU3MjMyLjM2NTE0LCJleHAiOjMzMjg1NzY2MDMyLjM2MzQsInN1YiI6Ijc2NzA5MDg2IiwiaXNzIjoiaHR0cHM6Ly9tZXRhLndpa2ltZWRpYS5vcmciLCJyYXRlbGltaXQiOnsicmVxdWVzdHNfcGVyX3VuaXQiOjUwMDAsInVuaXQiOiJIT1VSIn0sInNjb3BlcyI6WyJiYXNpYyJdfQ.qWYa0FklGUYrZbc9FOrPQvjm3sbU_Ei0k6dSRBAU-YKl-bouy_VDAU73G6kBrO27CDMyuWeDSPHW8Nb6N29QWw8bKMDj8edlcigIWhc-IbngwwOsqixQ9K1pwQHooYktyXgeRprtq3OuMnDOO-7uIb6DZflR1AtV9ZDivWkcxobaMzA5XgiEzLNI5L8GtDX_dTXb8tjHGcUVyDdzeoZIZ8RGKybIDJUaayVUYkTVGvak8LEaKNFOE-qyW8y3G5MjwZVGrv7eKtr5q-mQYvQvIVqF98FByD8T2zNeHwsVz1wxUDnYoIpvYqBEwmssPaDhqAFmakiqd49TVJ7_FJfQO-IYfGDG0i4EN_66n7stuPFFLkj5a7NEInNnIXibBMAiNWVlr-4LADUwdLA-UskcJETlbU0BWPCZetkBB7ZVm0TCSy35UFt8s4TZ4QlRc-8Bew8cCUauSFIvBWSK4Xb95ehCI5rW9FoA9JPdX2uquEWLMUlv3dSRTbpRbmMVOQ2ttLqlcA18K6qf0krA4h2WuS18Rmy8YBPhQZTAA-ISoeTSrgMKSDF4vUEhCm0MbimYhvc2SPExv5JCFA00uP6n_oaTFl_i6BkO9TE5s26K7tjKbUHqmCN7k3ABBpHK-bUrwC3dnYoA3ziZU1OQKtU2a7BmeXNXUUc9981DLWTaRGo\"\n",
    "\n",
    "# Function to get the ORES quality score for an article using the provided revision_id\n",
    "def process_article_quality_score(revision_id, article_title):\n",
    "    # Print progress\n",
    "    print(f\"Processing article: {article_title}\")\n",
    "    \n",
    "    # Get the ORES quality score using the revision_id\n",
    "    if revision_id is not None:\n",
    "        try:\n",
    "            revision_id = int(revision_id)  # Ensure revision_id is an integer\n",
    "            quality_score = request_ores_score_per_article(revision_id, email_address, access_token)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting ORES quality score for {article_title} with revision ID {revision_id}: {e}\")\n",
    "            quality_score = None\n",
    "    else:\n",
    "        quality_score = None\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through each article in the dataframe\n",
    "for index, row in politicians_df.iterrows():\n",
    "    article_title = row['name']\n",
    "    revision_id = row['revision_id']\n",
    "    \n",
    "    # Get the ORES quality score for the article using the revision_id\n",
    "    quality_score = process_article_quality_score(revision_id, article_title)\n",
    "    \n",
    "    # Append results as a dictionary to the list\n",
    "    results.append({\n",
    "        'name': article_title,\n",
    "        'revision_id': revision_id,\n",
    "        'quality_score': quality_score\n",
    "    })\n",
    "\n",
    "# Convert the list of results into a new dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "results_df.to_csv('politicians_by_country_with_ores_probs_AUG.2024.csv', index=False)\n",
    "\n",
    "# Display a confirmation message\n",
    "print(\"Results have been saved to 'politicians_by_country_with_ores_probs_AUG.2024.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12895f-dbb7-4566-a02e-816856919a86",
   "metadata": {},
   "source": [
    "## Attempt to make it quicker using Batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea628e4a-1611-45ac-81b5-ae6147ff1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50  # Number of articles to process in each batch\n",
    "\n",
    "def request_pageinfo_batch(article_titles):\n",
    "    titles = '|'.join(article_titles)  # Join titles with '|'\n",
    "    request_template = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "    request_template['titles'] = titles\n",
    "    \n",
    "    try:\n",
    "        time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(API_ENWIKIPEDIA_ENDPOINT, headers=REQUEST_HEADERS, params=request_template)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with batch request: {e}\")\n",
    "        return None\n",
    "\n",
    "# Match the tables that we will be using for this batch processing\n",
    "politicians_df_batch = polititians_df\n",
    "\n",
    "# Batch processing of articles\n",
    "for i in range(0, len(politicians_df_batch), BATCH_SIZE):\n",
    "    batch_df = politicians_df_batch.iloc[i:i+BATCH_SIZE]\n",
    "    titles = batch_df['name'].tolist() \n",
    "    print(f\"Processing batch: {titles[:5]}...\")  # Print first 5 article names from the batch for reference\n",
    "    \n",
    "    page_info = request_pageinfo_batch(titles)\n",
    "    if page_info:\n",
    "        pages_data = page_info.get('query', {}).get('pages', {})\n",
    "        for idx, title in enumerate(titles):\n",
    "            try:\n",
    "                page_info_details = next((v for k, v in pages_data.items() if v.get('title') == title), None)\n",
    "                if page_info_details:\n",
    "                    revision_id = page_info_details.get('lastrevid', None)\n",
    "                    if revision_id:\n",
    "                        politicians_df_batch.at[batch_df.index[idx], 'revision_id'] = revision_id\n",
    "                        \n",
    "                        # Get ORES prediction\n",
    "                        quality = get_ores_prediction(revision_id)\n",
    "                        if quality:\n",
    "                            politicians_df_batch.at[batch_df.index[idx], 'quality_prediction'] = quality\n",
    "                        else:\n",
    "                            errors.append(title)\n",
    "                    else:\n",
    "                        errors.append(title)\n",
    "            except Exception as e:\n",
    "                errors.append(title)\n",
    "                print(f\"Error processing {title}: {e}\")\n",
    "    else:\n",
    "        errors.extend(titles)  # Add all titles to error log if the batch fails\n",
    "\n",
    "# Saving the final output to a CSV file\n",
    "output_filename = 'politicians_with_ores_predictions_1.csv'\n",
    "politicians_df_batch.to_csv(output_filename, index=False)\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "\n",
    "# Saving the errors to a log file\n",
    "error_log_filename = 'ores_errors_log_1.txt'\n",
    "with open(error_log_filename, 'w') as f:\n",
    "    for article in errors:\n",
    "        f.write(f\"{article}\\n\")\n",
    "print(f\"Errors logged to {error_log_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a19591-0d67-4c57-9734-f93ec4451a41",
   "metadata": {},
   "source": [
    "As we do not want to do the crawl again for time sakes, we are going to load the extracted CSV file from repository called `politicians_with_ores_predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "12375edd-bdec-492c-8687-1278d78774dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load politicians data with scores\n",
    "politicians_df_batch = pd.read_csv('data/politicians_with_ores_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0a6a47da-9759-461b-b7ec-d15a67421214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles missing ORES scores: 8\n",
      "Error rate: 0.11%\n"
     ]
    }
   ],
   "source": [
    "# Log articles where quality_score is None\n",
    "missing_scores = politicians_df_batch[politicians_df_batch['quality_prediction'].isnull()]\n",
    "print(f\"Number of articles missing ORES scores: {len(missing_scores)}\")\n",
    "\n",
    "# Calculate the error rate for missing ORES scores\n",
    "error_rate = len(missing_scores) / len(politicians_df_batch)\n",
    "print(f\"Error rate: {error_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f29d11e-ed7b-4a1c-8931-a16d8667c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'url', 'country', 'revision_id', 'quality_prediction'], dtype='object')\n",
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country   revision_id quality_prediction  \n",
      "0  Afghanistan  1.233203e+09              Start  \n",
      "1  Afghanistan  1.230460e+09                  B  \n",
      "2  Afghanistan  1.225662e+09              Start  \n",
      "3  Afghanistan  1.234742e+09               Stub  \n",
      "4  Afghanistan  1.195651e+09              Start  \n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the dataframe\n",
    "print(politicians_df_batch.columns)\n",
    "\n",
    "# Preview the dataframe to verify the column\n",
    "print(politicians_df_batch.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a6e29d65-38fd-4692-90de-0c2b4d2ace3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created successfully:\n",
      "- wp_countries-no_match.txt\n",
      "- wp_politicians_by_country.csv\n"
     ]
    }
   ],
   "source": [
    "# Split the population data into regions and countries\n",
    "regions_df = population_df[population_df['Geography'].str.isupper()]\n",
    "countries_df = population_df[~population_df['Geography'].str.isupper()]\n",
    "\n",
    "# Assign each country to its nearest region (use the previous function)\n",
    "country_region_mapping = []\n",
    "current_region = None\n",
    "\n",
    "for idx, row in population_df.iterrows():\n",
    "    if row['Geography'].isupper():\n",
    "        current_region = row['Geography']  # Track the most recent region\n",
    "    else:\n",
    "        country_region_mapping.append({\n",
    "            'country': row['Geography'].lower(),  # Convert to lowercase for consistency\n",
    "            'region': current_region,\n",
    "            'population': row['Population']\n",
    "        })\n",
    "\n",
    "# Convert the country-region mapping into a DataFrame\n",
    "country_region_df = pd.DataFrame(country_region_mapping)\n",
    "\n",
    "# Prepare the politician data for merging\n",
    "politicians_df_batch['country'] = politicians_df_batch['country'].str.lower()  # Convert to lowercase\n",
    "\n",
    "# Merge the politician data with the country-region mapping\n",
    "merged_df = pd.merge(politicians_df_batch, country_region_df, on='country', how='left')\n",
    "\n",
    "# Identify countries that did not match between the datasets\n",
    "no_match_df = merged_df[merged_df['population'].isnull()]['country'].unique()\n",
    "\n",
    "# Save the list of unmatched countries to 'wp_countries-no_match.txt'\n",
    "with open('wp_countries-no_match.txt', 'w') as f:\n",
    "    for country in no_match_df:\n",
    "        f.write(f\"{country}\\n\")\n",
    "\n",
    "# Filter out the unmatched countries (remove NaN population entries)\n",
    "merged_df = merged_df[merged_df['population'].notnull()]\n",
    "\n",
    "# Select and rename the required columns for the final CSV\n",
    "final_df = merged_df[['country', 'region', 'population', 'name', 'revision_id', 'quality_prediction']]\n",
    "final_df = final_df.rename(columns={\n",
    "    'name': 'article_title',\n",
    "    'quality_prediction': 'article_quality'\n",
    "})\n",
    "\n",
    "# Save the consolidated dataset to 'wp_politicians_by_country.csv'\n",
    "final_df.to_csv('data/wp_politicians_by_country.csv', index=False)\n",
    "\n",
    "print(\"Files created successfully:\")\n",
    "print(\"- wp_countries-no_match.txt\")\n",
    "print(\"- wp_politicians_by_country.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b2f5c-c17b-431c-8449-2fda79850ae6",
   "metadata": {},
   "source": [
    "Some of our countries have population counts of zero, which is not true, hence I looked up the following countries populations and entered the, manually using Wikipedia:\n",
    "\n",
    "* [Monaco](https://en.wikipedia.org/wiki/Monaco)\n",
    "* [Tuvalu](https://en.wikipedia.org/wiki/Tuvalu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4daa5584-a7dd-4bd5-9e55-1d895ad8ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set the population values for Tuvalu and Monaco\n",
    "final_df.loc[final_df['country'] == 'tuvalu', 'population'] = 0.0119  # In millions (11,900 people)\n",
    "final_df.loc[final_df['country'] == 'monaco', 'population'] = 0.03836  # In millions (38,369 people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1db95-99dc-4f10-870a-b178aa554722",
   "metadata": {},
   "source": [
    "## Calculate Articles-per-Capita\n",
    "This is the ratio of the total number of articles per country or region to the population (remember that population is in millions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a2559b4c-66c8-4440-8a4b-fa187531f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       country  population  articles_per_capita  total_articles\n",
      "0  afghanistan        42.4             2.004717              85\n",
      "1  afghanistan        42.4             2.004717              85\n",
      "2  afghanistan        42.4             2.004717              85\n",
      "3  afghanistan        42.4             2.004717              85\n",
      "4  afghanistan        42.4             2.004717              85\n"
     ]
    }
   ],
   "source": [
    "# Calculate total articles-per-capita\n",
    "final_df['total_articles'] = final_df.groupby('country')['article_title'].transform('count')\n",
    "final_df['articles_per_capita'] = final_df['total_articles'] / final_df['population']\n",
    "\n",
    "# Show top 5 rows of the updated dataframe\n",
    "print(final_df[['country', 'population', 'articles_per_capita', 'total_articles']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73314dae-31e1-47dd-a88a-4ffa60715e1e",
   "metadata": {},
   "source": [
    "## Calculate High-Quality Articles-per-Capita\n",
    "Now filter the dataset to consider only the high-quality articles (FA or GA) and calculate the high-quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "de06a77f-1cdf-4171-8920-4b54686f10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for high-quality articles (FA and GA)\n",
    "high_quality_df = final_df[final_df['article_quality'].isin(['FA', 'GA'])].copy()\n",
    "\n",
    "# Calculate high-quality articles-per-capita\n",
    "high_quality_df['total_high_quality_articles'] = high_quality_df.groupby('country')['article_title'].transform('count')\n",
    "high_quality_df['high_quality_per_capita'] = high_quality_df['total_high_quality_articles'] / high_quality_df['population']\n",
    "\n",
    "# Group by country and get the first instance of each country\n",
    "country_high_quality_df = high_quality_df[['country', 'total_articles', 'population', 'high_quality_per_capita']].drop_duplicates(subset=['country'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dde60-40e8-4457-8012-693d71def40c",
   "metadata": {},
   "source": [
    "## 1. Top 10 Countries by Total Articles per Capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7d19f959-6288-494f-b85f-0e362810e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             country  total_articles  population  \\\n",
      "284              antigua and barbuda              33     0.10000   \n",
      "4260                          monaco              10     0.03836   \n",
      "4178  federated states of micronesia              14     0.10000   \n",
      "4135                marshall islands              13     0.10000   \n",
      "6598                           tonga              10     0.10000   \n",
      "6749                          tuvalu               1     0.01190   \n",
      "699                         barbados              25     0.30000   \n",
      "4279                      montenegro              36     0.60000   \n",
      "5590                      seychelles               6     0.10000   \n",
      "856                           bhutan              44     0.80000   \n",
      "\n",
      "      articles_per_capita  \n",
      "284                330.00  \n",
      "4260               260.69  \n",
      "4178               140.00  \n",
      "4135               130.00  \n",
      "6598               100.00  \n",
      "6749                84.03  \n",
      "699                 83.33  \n",
      "4279                60.00  \n",
      "5590                60.00  \n",
      "856                 55.00  \n"
     ]
    }
   ],
   "source": [
    "# Calculate total articles-per-capita for each country\n",
    "final_df['articles_per_capita'] = round(final_df.groupby('country')['article_title'].transform('count') / (final_df['population']), 2)\n",
    "\n",
    "# Group by country and get the first instance of each country\n",
    "country_coverage_df = final_df[['country', 'total_articles', 'population', 'articles_per_capita']].drop_duplicates(subset=['country'])\n",
    "\n",
    "# Sort by articles_per_capita and select the top 10\n",
    "top_10_countries_by_coverage = country_coverage_df.sort_values(by='articles_per_capita', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 countries by total articles per capita\n",
    "print(top_10_countries_by_coverage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad55c9-ce08-4a0b-af69-dab4c93a4f34",
   "metadata": {},
   "source": [
    "This table shows the top 10 countries with the highest number of Wikipedia articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34267122-b6ee-4634-a625-7e01f335e973",
   "metadata": {},
   "source": [
    "## 2. Bottom 10 Countries by Total Articles per Capita (in ascending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9da4531d-4152-4634-b17c-530fce513138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            country  total_articles  population  articles_per_capita\n",
      "1454          china              16      1411.3                 0.01\n",
      "2699          india             151      1428.6                 0.11\n",
      "2424          ghana               4        34.1                 0.12\n",
      "5475   saudi arabia               5        36.9                 0.14\n",
      "7083         zambia               3        20.2                 0.15\n",
      "4744         norway               1         5.5                 0.18\n",
      "3125         israel               2         9.8                 0.20\n",
      "2009          egypt              32       105.2                 0.30\n",
      "3275  cote d'ivoire              10        30.9                 0.32\n",
      "4379     mozambique              12        33.9                 0.35\n"
     ]
    }
   ],
   "source": [
    "# Sort by articles_per_capita in ascending order and select the bottom 10\n",
    "bottom_10_countries_by_coverage = country_coverage_df.sort_values(by='articles_per_capita', ascending=True).head(10)\n",
    "\n",
    "# Display the bottom 10 countries by total articles per capita\n",
    "print(bottom_10_countries_by_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8203e1d-84c9-4bf0-b419-6d4dfae12141",
   "metadata": {},
   "source": [
    "This table shows the bottom 10 countries with the lowest number of Wikipedia articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ce6c9-c027-407b-822f-bdbea1966bbd",
   "metadata": {},
   "source": [
    "## 3. Top 10 Countries by High-Quality Articles (Highest High-Quality Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4deaf8e9-c4e1-4434-92dc-ade011bf9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    country  total_articles  population  \\\n",
      "4279             montenegro              36         0.6   \n",
      "3903             luxembourg              27         0.7   \n",
      "85                  albania              70         2.7   \n",
      "3667                 kosovo              26         1.7   \n",
      "4089               maldives              33         0.6   \n",
      "3851              lithuania              58         2.9   \n",
      "1726                croatia              65         3.8   \n",
      "2551                 guyana              17         0.8   \n",
      "4874  palestinian territory              61         5.5   \n",
      "5663               slovenia              38         2.1   \n",
      "\n",
      "      high_quality_per_capita  \n",
      "4279                 5.000000  \n",
      "3903                 2.857143  \n",
      "85                   2.592593  \n",
      "3667                 2.352941  \n",
      "4089                 1.666667  \n",
      "3851                 1.379310  \n",
      "1726                 1.315789  \n",
      "2551                 1.250000  \n",
      "4874                 1.090909  \n",
      "5663                 0.952381  \n"
     ]
    }
   ],
   "source": [
    "# Sort by high_quality_per_capita and select the top 10\n",
    "top_10_countries_by_high_quality = country_high_quality_df.sort_values(by='high_quality_per_capita', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 countries by high quality articles per capita\n",
    "print(top_10_countries_by_high_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d52298-d139-42d4-8306-cd20947cea1c",
   "metadata": {},
   "source": [
    "This table shows the top 10 countries with the highest number of high-quality (FA or GA) Wikipedia articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474272e-91bf-411a-bd5d-617b994c08e4",
   "metadata": {},
   "source": [
    "## 4. Bottom 10 Countries by High-Quality Articles (Lowest High-Quality Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c085cf3f-5f28-41ef-9e12-f2db74c101bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         country  total_articles  population  high_quality_per_capita\n",
      "636   bangladesh              74       173.5                 0.005764\n",
      "2025       egypt              32       105.2                 0.009506\n",
      "2101    ethiopia              44       126.5                 0.015810\n",
      "3318       japan             117       124.5                 0.016064\n",
      "4775    pakistan              94       240.5                 0.016632\n",
      "1501    colombia              78        52.2                 0.019157\n",
      "1617    congo dr              53       102.3                 0.019550\n",
      "7049     vietnam              36        98.9                 0.020222\n",
      "6776      uganda              65        48.6                 0.020576\n",
      "176      algeria              71        46.8                 0.021368\n"
     ]
    }
   ],
   "source": [
    "# Sort by high_quality_per_capita in ascending order and select the bottom 10\n",
    "bottom_10_countries_by_high_quality = country_high_quality_df.sort_values(by='high_quality_per_capita', ascending=True).head(10)\n",
    "\n",
    "# Display the bottom 10 countries by high quality articles per capita\n",
    "print(bottom_10_countries_by_high_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0ce9b-aa24-442d-b134-7df088a1ac84",
   "metadata": {},
   "source": [
    "This table displays the bottom 10 countries with the lowest number of high-quality Wikipedia articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750a952-2941-4319-9814-023b8baa39f7",
   "metadata": {},
   "source": [
    "## 5. Geographic Regions by Total Coverage (Ranked by Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1a549191-82eb-44e8-b9be-9bb594050989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions by total articles per capita:\n",
      "             region  total_articles  Population  articles_per_capita\n",
      "14  SOUTHERN EUROPE             797       152.0             5.243421\n",
      "0         CARIBBEAN             219        44.0             4.977273\n",
      "17   WESTERN EUROPE             498       199.0             2.502513\n",
      "5    EASTERN EUROPE             709       285.0             2.487719\n",
      "16     WESTERN ASIA             610       299.0             2.040134\n",
      "8   NORTHERN EUROPE             191       108.0             1.768519\n",
      "13  SOUTHERN AFRICA             123        70.0             1.757143\n",
      "9           OCEANIA              72        45.0             1.600000\n",
      "4    EASTERN AFRICA             665       483.0             1.376812\n",
      "10    SOUTH AMERICA             569       426.0             1.335681\n",
      "2      CENTRAL ASIA             106        80.0             1.325000\n",
      "7   NORTHERN AFRICA             302       256.0             1.179688\n",
      "15   WESTERN AFRICA             515       442.0             1.165158\n",
      "6     MIDDLE AFRICA             231       202.0             1.143564\n",
      "1   CENTRAL AMERICA             188       182.0             1.032967\n",
      "12   SOUTHEAST ASIA             396       682.0             0.580645\n",
      "11       SOUTH ASIA             670      2029.0             0.330212\n",
      "3         EAST ASIA             152      1648.0             0.092233\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Use the population data from regions_df for each region\n",
    "# regions_df contains the population for each region as per the table provided\n",
    "\n",
    "# Make a copy of regions_df to avoid SettingWithCopyWarning and standardize region names\n",
    "regions_df = regions_df.copy()\n",
    "\n",
    "# Standardize 'Geography' names in regions_df\n",
    "regions_df['Geography'] = regions_df['Geography'].str.strip().str.upper()\n",
    "\n",
    "# Step 2: Group by region and count the total number of articles per region from the final_df\n",
    "region_coverage_df = final_df.groupby('region').size().reset_index(name='total_articles')\n",
    "\n",
    "# Step 3: Merge region_coverage_df with regions_df to get the correct population data for each region\n",
    "region_coverage_df = pd.merge(region_coverage_df, regions_df, left_on='region', right_on='Geography')\n",
    "\n",
    "# Step 4: Calculate total articles-per-capita for each region using the population from regions_df\n",
    "region_coverage_df['articles_per_capita'] = region_coverage_df['total_articles'] / region_coverage_df['Population']\n",
    "\n",
    "# Step 5: Sort regions by articles-per-capita and display the result\n",
    "regions_by_total_coverage = region_coverage_df.sort_values(by='articles_per_capita', ascending=False)\n",
    "\n",
    "# Display the result: total articles, population, and articles-per-capita for each region\n",
    "print(\"Regions by total articles per capita:\")\n",
    "print(regions_by_total_coverage[['region', 'total_articles', 'Population', 'articles_per_capita']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e5574-3f9c-4bbd-bcfd-fd38c870bb45",
   "metadata": {},
   "source": [
    "This table shows a rank-ordered list of geographic regions by total articles per capita (highest to lowest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43749ddc-1b89-4aa9-a353-0d7423aa9dae",
   "metadata": {},
   "source": [
    "## 6. Geographic Regions by High-Quality Coverage (Ranked by High-Quality Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "674f8f33-4c23-41eb-a350-4da3cb2c0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions by high-quality articles per capita:\n",
      "             region  total_high_quality_articles  Population  \\\n",
      "14  SOUTHERN EUROPE                           53       152.0   \n",
      "0         CARIBBEAN                            9        44.0   \n",
      "5    EASTERN EUROPE                           38       285.0   \n",
      "13  SOUTHERN AFRICA                            8        70.0   \n",
      "17   WESTERN EUROPE                           21       199.0   \n",
      "16     WESTERN ASIA                           27       299.0   \n",
      "8   NORTHERN EUROPE                            9       108.0   \n",
      "7   NORTHERN AFRICA                           17       256.0   \n",
      "2      CENTRAL ASIA                            5        80.0   \n",
      "1   CENTRAL AMERICA                           10       182.0   \n",
      "10    SOUTH AMERICA                           19       426.0   \n",
      "6     MIDDLE AFRICA                            8       202.0   \n",
      "12   SOUTHEAST ASIA                           25       682.0   \n",
      "4    EASTERN AFRICA                           17       483.0   \n",
      "15   WESTERN AFRICA                           13       442.0   \n",
      "9           OCEANIA                            1        45.0   \n",
      "11       SOUTH ASIA                           21      2029.0   \n",
      "3         EAST ASIA                            3      1648.0   \n",
      "\n",
      "    high_quality_per_capita  \n",
      "14                 0.348684  \n",
      "0                  0.204545  \n",
      "5                  0.133333  \n",
      "13                 0.114286  \n",
      "17                 0.105528  \n",
      "16                 0.090301  \n",
      "8                  0.083333  \n",
      "7                  0.066406  \n",
      "2                  0.062500  \n",
      "1                  0.054945  \n",
      "10                 0.044601  \n",
      "6                  0.039604  \n",
      "12                 0.036657  \n",
      "4                  0.035197  \n",
      "15                 0.029412  \n",
      "9                  0.022222  \n",
      "11                 0.010350  \n",
      "3                  0.001820  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Use the population data from regions_df for each region\n",
    "# Ensure 'Geography' and 'region' are consistent for merging\n",
    "regions_df = regions_df.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "regions_df['Geography'] = regions_df['Geography'].str.strip().str.upper()  # Standardize region names\n",
    "\n",
    "# Step 2: Group by region and count the total number of high-quality articles per region from high_quality_df\n",
    "region_high_quality_df = high_quality_df.groupby('region').size().reset_index(name='total_high_quality_articles')\n",
    "\n",
    "# Step 3: Merge region_high_quality_df with regions_df to get the correct population data for each region\n",
    "region_high_quality_df = pd.merge(region_high_quality_df, regions_df, left_on='region', right_on='Geography')\n",
    "\n",
    "# Step 4: Calculate high-quality articles-per-capita for each region using the population from regions_df\n",
    "region_high_quality_df['high_quality_per_capita'] = region_high_quality_df['total_high_quality_articles'] / region_high_quality_df['Population']\n",
    "\n",
    "# Step 5: Sort regions by high-quality articles-per-capita and display the result\n",
    "regions_by_high_quality_coverage = region_high_quality_df.sort_values(by='high_quality_per_capita', ascending=False)\n",
    "\n",
    "# Display the result: total high-quality articles, population, and high-quality articles-per-capita for each region\n",
    "print(\"Regions by high-quality articles per capita:\")\n",
    "print(regions_by_high_quality_coverage[['region', 'total_high_quality_articles', 'Population', 'high_quality_per_capita']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a1880-e3ef-4941-b6fc-8d793eeab2c0",
   "metadata": {},
   "source": [
    "This table shows a rank-ordered list of geographic regions by high-quality (FA/GA) articles per capita."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
