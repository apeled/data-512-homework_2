{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10768d73-ba5e-47a1-8c4c-f303d52568b6",
   "metadata": {},
   "source": [
    "# Considering Bias in Data\n",
    "Author: Amit Peled\n",
    "\n",
    "Homework 2\n",
    "\n",
    "## Requesting ORES scores through LiftWing ML Service API\n",
    "\n",
    "Wikimedia Foundation (WMF) is reworking access to their APIs. It is likely in the coming years that all API access will require some kind of authentication, either through a simple key/token or through some version of OAuth. For now this is still a work in progress. You can follow the progress from their [API portal](https://api.wikimedia.org/wiki/Main_Page). Another on-going change is better control over API services in situations where those services require additional computational resources, beyond simply serving the text of a web page (i.e., the text of an article). Services like ORES that require running an ML model over the text of an article page is an example of a compute intensive API service.\n",
    "\n",
    "Wikimedia is implementing a new Machine Learning (ML) service infrastructure that they call [LiftWing](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing). Given that ORES already has several ML models that have been well used, ORES is the first set of APIs that are being moved to LiftWing.\n",
    "\n",
    "This example illustrates how to generate article quality estimates for article revisions using the LiftWing version of [ORES](https://www.mediawiki.org/wiki/ORES). The [ORES API documentation](https://ores.wikimedia.org) can be accessed from the main ORES page. The [ORES LiftWing documentation](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing/Usage) is very thin ... even thinner than the standard ORES documentation. Further, it is clear that some parameters have been renamed (e.g., \"revid\" in the old ORES API is now \"rev_id\" in the LiftWing ORES API).\n",
    "\n",
    "## Article Page Info MediaWiki API \n",
    "This notebook also illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). It requests summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024\n",
    "\n",
    "Note: This project was developed with the assistance of ChatGPT, which helped format code and organize the data and analysis in an efficient and clear manner. All content has been verified and tested by the author.\r",
    "d\n",
    "\n",
    "This notebook demonstrates how to use the LiftWing API to generate article quality estimates for Wikipedia article revisions using the ORES model.\n",
    "\n",
    "* ORES API Documentation: [ORES API documentation](https://ores.wikimedia.org)\n",
    "* LiftWing Documentation: [LiftWing ORES API Documentation](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing/Usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de19aa-cbcd-40bb-b9ad-8d258e351a5d",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "In this notebook, I aim to analyze the coverage and quality of Wikipedia articles about political figures across various countries. I will:\n",
    "\n",
    "- Retrieve article metadata and ORES quality predictions using the Wikimedia API and the LiftWing infrastructure.\n",
    "- Merge this data with population statistics for each country and geographic region.\n",
    "- Calculate per capita coverage for both articles and high-quality articles.\n",
    "- Analyze geographic patterns by producing tables that rank countries and regions by article coverage and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad8c6d-97c9-4bc4-9b7b-972ecf0b3645",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4879999-8f24-4218-b7b8-cb5a3cef8d8b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ab3468e-0de0-4324-b902-f8a049cc9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse, pandas as pd\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535fc4a-c319-486f-a2a2-1e9ef954d275",
   "metadata": {},
   "source": [
    "### Load CSV Data (Politicians and Population) \n",
    "These files are located in the repositiry of this project, and were provided to me by the teaching staff of DATA 512 for the purposes of this assingment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3320194d-5242-49ff-8c8c-78ce1d51aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country  \n",
      "0  Afghanistan  \n",
      "1  Afghanistan  \n",
      "2  Afghanistan  \n",
      "3  Afghanistan  \n",
      "4  Afghanistan  \n",
      "         Geography  Population\n",
      "0            WORLD      8009.0\n",
      "1           AFRICA      1453.0\n",
      "2  NORTHERN AFRICA       256.0\n",
      "3          Algeria        46.8\n",
      "4            Egypt       105.2\n"
     ]
    }
   ],
   "source": [
    "# Load politicians data\n",
    "politicians_df = pd.read_csv('data/politicians_by_country_AUG.2024.csv')\n",
    "\n",
    "# Load population data\n",
    "population_df = pd.read_csv('data/population_by_country_AUG.2024.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe\n",
    "print(politicians_df.head())\n",
    "print(population_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df06c94-9172-4fc1-a180-27c772a7b6cc",
   "metadata": {},
   "source": [
    "### Data Cleaning and Handling Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0702615-79bf-4c2e-8241-e30d4e8e6701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in Politicians Data: 0\n",
      "Missing values in Politicians Data: name       0\n",
      "url        0\n",
      "country    0\n",
      "dtype: int64\n",
      "          Geography  Population\n",
      "0             WORLD      8009.0\n",
      "1            AFRICA      1453.0\n",
      "2   NORTHERN AFRICA       256.0\n",
      "10   WESTERN AFRICA       442.0\n",
      "27   EASTERN AFRICA       483.0\n",
      "  Geography  Population\n",
      "3   Algeria        46.8\n",
      "4     Egypt       105.2\n",
      "5     Libya         6.9\n",
      "6   Morocco        37.0\n",
      "7     Sudan        48.1\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in politicians data\n",
    "print(\"Duplicates in Politicians Data:\", politicians_df.duplicated().sum())\n",
    "\n",
    "# Check for missing values in politicians data\n",
    "print(\"Missing values in Politicians Data:\", politicians_df.isnull().sum())\n",
    "\n",
    "# Split population data into regions and countries\n",
    "regions_df = population_df[population_df['Geography'].str.isupper()]\n",
    "countries_df = population_df[~population_df['Geography'].str.isupper()]\n",
    "\n",
    "# Check the first few rows to ensure correct splitting\n",
    "print(regions_df.head())\n",
    "print(countries_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05aca0-886f-477f-90ba-53998e8d0c85",
   "metadata": {},
   "source": [
    "## Making API Requests for Article Metadata and ORES Quality Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb6b33-64d3-4d44-a8aa-8b6f376fbca4",
   "metadata": {},
   "source": [
    "### Constants and Setup for API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "369355f7-4137-4fd8-b9c2-7c39dfd694f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "ORES_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/enwiki\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"apeled@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<apeled@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d6544-f214-46df-a9ce-22ed379bbf2b",
   "metadata": {},
   "source": [
    "### Function to Get Wikipedia Article Metadata (Page Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "244f2ed1-2652-4abe-a7e4-63c81d6dd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e46e5-750c-4d64-bbe2-3915199aa4b5",
   "metadata": {},
   "source": [
    "### Function to Get ORES Article Quality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "361a5cd0-21a9-4caf-9a49-85bc99c7cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aebe7b-2073-4bdd-af54-c51ebc4f6f36",
   "metadata": {},
   "source": [
    "## Running Requests for Each Article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bb378-31a8-4b1e-80f1-cbd79f9f2078",
   "metadata": {},
   "source": [
    "### Retrieve Revision IDs for Each Article and then the ORES scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056475b8-3e3a-46b0-bbbd-1678b76d16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your actual email and ORES API access token\n",
    "email_address = \"apgsw30@gmail.com\"\n",
    "access_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiJjM2UzMTkyZGE4YWNmNDc5ZTgzODA5MDkwZWY0NTBjMCIsImp0aSI6ImIwY2NlMGM3ZGQwNGRlYmQ0Yjc1ZDc4ZjRiOGM1ZmU2ZjFiZWQ5MmJiNTc4NmNjYTQxM2I3MTI0ZTIwNjBhMDcyNjBiNDViMGZkYTA3NTUyIiwiaWF0IjoxNzI4ODU3MjMyLjM2NTEzNywibmJmIjoxNzI4ODU3MjMyLjM2NTE0LCJleHAiOjMzMjg1NzY2MDMyLjM2MzQsInN1YiI6Ijc2NzA5MDg2IiwiaXNzIjoiaHR0cHM6Ly9tZXRhLndpa2ltZWRpYS5vcmciLCJyYXRlbGltaXQiOnsicmVxdWVzdHNfcGVyX3VuaXQiOjUwMDAsInVuaXQiOiJIT1VSIn0sInNjb3BlcyI6WyJiYXNpYyJdfQ.qWYa0FklGUYrZbc9FOrPQvjm3sbU_Ei0k6dSRBAU-YKl-bouy_VDAU73G6kBrO27CDMyuWeDSPHW8Nb6N29QWw8bKMDj8edlcigIWhc-IbngwwOsqixQ9K1pwQHooYktyXgeRprtq3OuMnDOO-7uIb6DZflR1AtV9ZDivWkcxobaMzA5XgiEzLNI5L8GtDX_dTXb8tjHGcUVyDdzeoZIZ8RGKybIDJUaayVUYkTVGvak8LEaKNFOE-qyW8y3G5MjwZVGrv7eKtr5q-mQYvQvIVqF98FByD8T2zNeHwsVz1wxUDnYoIpvYqBEwmssPaDhqAFmakiqd49TVJ7_FJfQO-IYfGDG0i4EN_66n7stuPFFLkj5a7NEInNnIXibBMAiNWVlr-4LADUwdLA-UskcJETlbU0BWPCZetkBB7ZVm0TCSy35UFt8s4TZ4QlRc-8Bew8cCUauSFIvBWSK4Xb95ehCI5rW9FoA9JPdX2uquEWLMUlv3dSRTbpRbmMVOQ2ttLqlcA18K6qf0krA4h2WuS18Rmy8YBPhQZTAA-ISoeTSrgMKSDF4vUEhCm0MbimYhvc2SPExv5JCFA00uP6n_oaTFl_i6BkO9TE5s26K7tjKbUHqmCN7k3ABBpHK-bUrwC3dnYoA3ziZU1OQKtU2a7BmeXNXUUc9981DLWTaRGo\"\n",
    "\n",
    "# Function to retrieve the ORES quality score for each article\n",
    "def process_article(article_title, email_address, access_token):\n",
    "    # Get the page info for the article (to get revision ID)\n",
    "    page_info = request_pageinfo_per_article(article_title=article_title)\n",
    "    if page_info:\n",
    "        pages_data = page_info.get('query', {}).get('pages', {})\n",
    "        for page_id, page_details in pages_data.items():\n",
    "            revision_id = page_details.get('lastrevid', None)\n",
    "            if revision_id:\n",
    "                # Get the ORES score using the revision ID\n",
    "                quality_score = request_ores_score_per_article(revision_id, email_address, access_token)\n",
    "                return revision_id, quality_score\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Initialize list to store errors\n",
    "errors = []\n",
    "\n",
    "# Process articles one by one\n",
    "for index, row in politicians_df.iterrows():\n",
    "    article_title = row['name']\n",
    "    print(f\"Processing article: {article_title}\")\n",
    "    \n",
    "    # Get the ORES quality score for the article\n",
    "    revision_id, quality_score = process_article(article_title, email_address, access_token)\n",
    "    \n",
    "    if revision_id and quality_score:\n",
    "        politicians_df.at[index, 'revision_id'] = revision_id\n",
    "        politicians_df.at[index, 'quality_prediction'] = quality_score\n",
    "    else:\n",
    "        errors.append(article_title)\n",
    "\n",
    "# Save the final output to a CSV file\n",
    "output_filename = 'politicians_with_ores_predictions.csv'\n",
    "politicians_df.to_csv(output_filename, index=False)\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "\n",
    "# Save the errors to a log file\n",
    "error_log_filename = 'ores_errors_log.txt'\n",
    "with open(error_log_filename, 'w') as f:\n",
    "    for article in errors:\n",
    "        f.write(f\"{article}\\n\")\n",
    "print(f\"Errors logged to {error_log_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a19591-0d67-4c57-9734-f93ec4451a41",
   "metadata": {},
   "source": [
    "As we do not want to do the crawl again for time sakes, we are going to load the extracted CSV file from repository called `politicians_with_ores_predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12375edd-bdec-492c-8687-1278d78774dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load politicians data with scores\n",
    "politicians_with_ores_predictions = pd.read_csv('data/politicians_with_ores_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a6a47da-9759-461b-b7ec-d15a67421214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles missing ORES scores: 8\n",
      "Error rate: 0.11%\n"
     ]
    }
   ],
   "source": [
    "# Log articles where quality_score is None\n",
    "missing_scores = politicians_with_ores_predictions[politicians_with_ores_predictions['quality_prediction'].isnull()]\n",
    "print(f\"Number of articles missing ORES scores: {len(missing_scores)}\")\n",
    "\n",
    "# Calculate the error rate for missing ORES scores\n",
    "error_rate = len(missing_scores) / len(politicians_df_batch)\n",
    "print(f\"Error rate: {error_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f29d11e-ed7b-4a1c-8931-a16d8667c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'url', 'country', 'revision_id', 'quality_prediction'], dtype='object')\n",
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country   revision_id quality_prediction  \n",
      "0  Afghanistan  1.233203e+09              Start  \n",
      "1  Afghanistan  1.230460e+09                  B  \n",
      "2  Afghanistan  1.225662e+09              Start  \n",
      "3  Afghanistan  1.234742e+09               Stub  \n",
      "4  Afghanistan  1.195651e+09              Start  \n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the dataframe\n",
    "print(politicians_with_ores_predictions.columns)\n",
    "\n",
    "# Preview the dataframe to verify the column\n",
    "print(politicians_with_ores_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f897c15-badd-4eaa-a899-e03b415f5291",
   "metadata": {},
   "source": [
    "## Combining the Datasets\n",
    "\n",
    "Merge the politicain data I just obtained with population statistics for each country and geographic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6e29d65-38fd-4692-90de-0c2b4d2ace3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created successfully:\n",
      "- wp_countries-no_match.txt\n",
      "- wp_politicians_by_country.csv\n"
     ]
    }
   ],
   "source": [
    "# Split the population data into regions and countries\n",
    "regions_df = population_df[population_df['Geography'].str.isupper()]\n",
    "countries_df = population_df[~population_df['Geography'].str.isupper()]\n",
    "\n",
    "# Assign each country to its nearest region (use the previous function)\n",
    "country_region_mapping = []\n",
    "current_region = None\n",
    "\n",
    "for idx, row in population_df.iterrows():\n",
    "    if row['Geography'].isupper():\n",
    "        current_region = row['Geography']  # Track the most recent region\n",
    "    else:\n",
    "        country_region_mapping.append({\n",
    "            'country': row['Geography'].lower(),  # Convert to lowercase for consistency\n",
    "            'region': current_region,\n",
    "            'population': row['Population']\n",
    "        })\n",
    "\n",
    "# Convert the country-region mapping into a DataFrame\n",
    "country_region_df = pd.DataFrame(country_region_mapping)\n",
    "\n",
    "# Merge the politician data with the country-region mapping\n",
    "politicians_with_ores_predictions['country'] = politicians_with_ores_predictions['country'].str.lower()  # Convert to lowercase\n",
    "merged_df = pd.merge(politicians_with_ores_predictions, country_region_df, on='country', how='left')\n",
    "\n",
    "# Identify countries that did not match between the datasets\n",
    "no_match_df = merged_df[merged_df['population'].isnull()]['country'].unique()\n",
    "\n",
    "# Save the list of unmatched countries to 'wp_countries-no_match.txt'\n",
    "with open('wp_countries-no_match.txt', 'w') as f:\n",
    "    for country in no_match_df:\n",
    "        f.write(f\"{country}\\n\")\n",
    "\n",
    "# Filter out the unmatched countries (remove NaN population entries)\n",
    "merged_df = merged_df[merged_df['population'].notnull()]\n",
    "\n",
    "# Select and rename the required columns for the final CSV\n",
    "final_df = merged_df[['country', 'region', 'population', 'name', 'revision_id', 'quality_prediction']]\n",
    "final_df = final_df.rename(columns={\n",
    "    'name': 'article_title',\n",
    "    'quality_prediction': 'article_quality'\n",
    "})\n",
    "\n",
    "# Save the consolidated dataset to 'wp_politicians_by_country.csv'\n",
    "final_df.to_csv('data/wp_politicians_by_country.csv', index=False)\n",
    "\n",
    "print(\"Files created successfully:\")\n",
    "print(\"- wp_countries-no_match.txt\")\n",
    "print(\"- wp_politicians_by_country.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b2f5c-c17b-431c-8449-2fda79850ae6",
   "metadata": {},
   "source": [
    "Some of our countries have population counts of zero, which is not true, hence I looked up the following countries populations and entered the, manually using Wikipedia:\n",
    "\n",
    "* [Monaco](https://en.wikipedia.org/wiki/Monaco)\n",
    "* [Tuvalu](https://en.wikipedia.org/wiki/Tuvalu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4daa5584-a7dd-4bd5-9e55-1d895ad8ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set the population values for Tuvalu and Monaco\n",
    "final_df.loc[final_df['country'] == 'tuvalu', 'population'] = 0.0119  # In millions (11,900 people)\n",
    "final_df.loc[final_df['country'] == 'monaco', 'population'] = 0.03836  # In millions (38,369 people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1db95-99dc-4f10-870a-b178aa554722",
   "metadata": {},
   "source": [
    "## Calculate Articles-per-Capita\n",
    "This is the ratio of the total number of articles per country or region to the population (remember that population is in millions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2559b4c-66c8-4440-8a4b-fa187531f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       country  population  articles_per_capita  total_articles\n",
      "0  afghanistan        42.4             2.004717              85\n",
      "1  afghanistan        42.4             2.004717              85\n",
      "2  afghanistan        42.4             2.004717              85\n",
      "3  afghanistan        42.4             2.004717              85\n",
      "4  afghanistan        42.4             2.004717              85\n"
     ]
    }
   ],
   "source": [
    "# Calculate total articles-per-capita\n",
    "final_df['total_articles'] = final_df.groupby('country')['article_title'].transform('count')\n",
    "final_df['articles_per_capita'] = final_df['total_articles'] / final_df['population']\n",
    "\n",
    "# Show top 5 rows of the updated dataframe\n",
    "print(final_df[['country', 'population', 'articles_per_capita', 'total_articles']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73314dae-31e1-47dd-a88a-4ffa60715e1e",
   "metadata": {},
   "source": [
    "## Calculate High-Quality Articles-per-Capita\n",
    "Now filter the dataset to consider only the high-quality articles (FA or GA) and calculate the high-quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de06a77f-1cdf-4171-8920-4b54686f10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for high-quality articles (FA and GA)\n",
    "high_quality_df = final_df[final_df['article_quality'].isin(['FA', 'GA'])].copy()\n",
    "\n",
    "# Calculate high-quality articles-per-capita\n",
    "high_quality_df['total_high_quality_articles'] = high_quality_df.groupby('country')['article_title'].transform('count')\n",
    "high_quality_df['high_quality_per_capita'] = high_quality_df['total_high_quality_articles'] / high_quality_df['population']\n",
    "\n",
    "# Group by country and get the first instance of each country\n",
    "country_high_quality_df = high_quality_df[['country', 'total_articles', 'population', 'high_quality_per_capita']].drop_duplicates(subset=['country'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dde60-40e8-4457-8012-693d71def40c",
   "metadata": {},
   "source": [
    "## 1. Top 10 Countries by Total Articles per Capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d19f959-6288-494f-b85f-0e362810e99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0b5d2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0b5d2_level0_col0\" class=\"col_heading level0 col0\" >country</th>\n",
       "      <th id=\"T_0b5d2_level0_col1\" class=\"col_heading level0 col1\" >total_articles</th>\n",
       "      <th id=\"T_0b5d2_level0_col2\" class=\"col_heading level0 col2\" >population</th>\n",
       "      <th id=\"T_0b5d2_level0_col3\" class=\"col_heading level0 col3\" >articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row0\" class=\"row_heading level0 row0\" >284</th>\n",
       "      <td id=\"T_0b5d2_row0_col0\" class=\"data row0 col0\" >antigua and barbuda</td>\n",
       "      <td id=\"T_0b5d2_row0_col1\" class=\"data row0 col1\" >33</td>\n",
       "      <td id=\"T_0b5d2_row0_col2\" class=\"data row0 col2\" >0.10</td>\n",
       "      <td id=\"T_0b5d2_row0_col3\" class=\"data row0 col3\" >330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row1\" class=\"row_heading level0 row1\" >4260</th>\n",
       "      <td id=\"T_0b5d2_row1_col0\" class=\"data row1 col0\" >monaco</td>\n",
       "      <td id=\"T_0b5d2_row1_col1\" class=\"data row1 col1\" >10</td>\n",
       "      <td id=\"T_0b5d2_row1_col2\" class=\"data row1 col2\" >0.04</td>\n",
       "      <td id=\"T_0b5d2_row1_col3\" class=\"data row1 col3\" >260.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row2\" class=\"row_heading level0 row2\" >4178</th>\n",
       "      <td id=\"T_0b5d2_row2_col0\" class=\"data row2 col0\" >federated states of micronesia</td>\n",
       "      <td id=\"T_0b5d2_row2_col1\" class=\"data row2 col1\" >14</td>\n",
       "      <td id=\"T_0b5d2_row2_col2\" class=\"data row2 col2\" >0.10</td>\n",
       "      <td id=\"T_0b5d2_row2_col3\" class=\"data row2 col3\" >140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row3\" class=\"row_heading level0 row3\" >4135</th>\n",
       "      <td id=\"T_0b5d2_row3_col0\" class=\"data row3 col0\" >marshall islands</td>\n",
       "      <td id=\"T_0b5d2_row3_col1\" class=\"data row3 col1\" >13</td>\n",
       "      <td id=\"T_0b5d2_row3_col2\" class=\"data row3 col2\" >0.10</td>\n",
       "      <td id=\"T_0b5d2_row3_col3\" class=\"data row3 col3\" >130.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row4\" class=\"row_heading level0 row4\" >6598</th>\n",
       "      <td id=\"T_0b5d2_row4_col0\" class=\"data row4 col0\" >tonga</td>\n",
       "      <td id=\"T_0b5d2_row4_col1\" class=\"data row4 col1\" >10</td>\n",
       "      <td id=\"T_0b5d2_row4_col2\" class=\"data row4 col2\" >0.10</td>\n",
       "      <td id=\"T_0b5d2_row4_col3\" class=\"data row4 col3\" >100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row5\" class=\"row_heading level0 row5\" >6749</th>\n",
       "      <td id=\"T_0b5d2_row5_col0\" class=\"data row5 col0\" >tuvalu</td>\n",
       "      <td id=\"T_0b5d2_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "      <td id=\"T_0b5d2_row5_col2\" class=\"data row5 col2\" >0.01</td>\n",
       "      <td id=\"T_0b5d2_row5_col3\" class=\"data row5 col3\" >84.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row6\" class=\"row_heading level0 row6\" >699</th>\n",
       "      <td id=\"T_0b5d2_row6_col0\" class=\"data row6 col0\" >barbados</td>\n",
       "      <td id=\"T_0b5d2_row6_col1\" class=\"data row6 col1\" >25</td>\n",
       "      <td id=\"T_0b5d2_row6_col2\" class=\"data row6 col2\" >0.30</td>\n",
       "      <td id=\"T_0b5d2_row6_col3\" class=\"data row6 col3\" >83.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row7\" class=\"row_heading level0 row7\" >4279</th>\n",
       "      <td id=\"T_0b5d2_row7_col0\" class=\"data row7 col0\" >montenegro</td>\n",
       "      <td id=\"T_0b5d2_row7_col1\" class=\"data row7 col1\" >36</td>\n",
       "      <td id=\"T_0b5d2_row7_col2\" class=\"data row7 col2\" >0.60</td>\n",
       "      <td id=\"T_0b5d2_row7_col3\" class=\"data row7 col3\" >60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row8\" class=\"row_heading level0 row8\" >5590</th>\n",
       "      <td id=\"T_0b5d2_row8_col0\" class=\"data row8 col0\" >seychelles</td>\n",
       "      <td id=\"T_0b5d2_row8_col1\" class=\"data row8 col1\" >6</td>\n",
       "      <td id=\"T_0b5d2_row8_col2\" class=\"data row8 col2\" >0.10</td>\n",
       "      <td id=\"T_0b5d2_row8_col3\" class=\"data row8 col3\" >60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0b5d2_level0_row9\" class=\"row_heading level0 row9\" >856</th>\n",
       "      <td id=\"T_0b5d2_row9_col0\" class=\"data row9 col0\" >bhutan</td>\n",
       "      <td id=\"T_0b5d2_row9_col1\" class=\"data row9 col1\" >44</td>\n",
       "      <td id=\"T_0b5d2_row9_col2\" class=\"data row9 col2\" >0.80</td>\n",
       "      <td id=\"T_0b5d2_row9_col3\" class=\"data row9 col3\" >55.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb565c0f790>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate total articles-per-capita for each country\n",
    "final_df['articles_per_capita'] = round(final_df.groupby('country')['article_title'].transform('count') / (final_df['population']), 2)\n",
    "\n",
    "# Group by country and get the first instance of each country\n",
    "country_coverage_df = final_df[['country', 'total_articles', 'population', 'articles_per_capita']].drop_duplicates(subset=['country'])\n",
    "\n",
    "# Sort by articles_per_capita and select the top 10\n",
    "top_10_countries_by_coverage = country_coverage_df.sort_values(by='articles_per_capita', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 countries by high quality articles per capita in a formatted table\n",
    "top_10_countries_by_coverage = top_10_countries_by_coverage.style.format({\n",
    "    'total_articles': '{:.0f}',\n",
    "    'population': '{:.2f}',                   # 2 decimal places for population (in millions)\n",
    "    'articles_per_capita': '{:.6f}'       # 6 decimal places for per capita ratios\n",
    "})\n",
    "\n",
    "# Show formatted table\n",
    "top_10_countries_by_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad55c9-ce08-4a0b-af69-dab4c93a4f34",
   "metadata": {},
   "source": [
    "This table shows the top 10 countries with the highest number of Wikipedia articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34267122-b6ee-4634-a625-7e01f335e973",
   "metadata": {},
   "source": [
    "## 2. Bottom 10 Countries by Total Articles per Capita (in ascending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9da4531d-4152-4634-b17c-530fce513138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e7d0a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e7d0a_level0_col0\" class=\"col_heading level0 col0\" >country</th>\n",
       "      <th id=\"T_e7d0a_level0_col1\" class=\"col_heading level0 col1\" >total_articles</th>\n",
       "      <th id=\"T_e7d0a_level0_col2\" class=\"col_heading level0 col2\" >population</th>\n",
       "      <th id=\"T_e7d0a_level0_col3\" class=\"col_heading level0 col3\" >articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row0\" class=\"row_heading level0 row0\" >1454</th>\n",
       "      <td id=\"T_e7d0a_row0_col0\" class=\"data row0 col0\" >china</td>\n",
       "      <td id=\"T_e7d0a_row0_col1\" class=\"data row0 col1\" >16</td>\n",
       "      <td id=\"T_e7d0a_row0_col2\" class=\"data row0 col2\" >1411.30</td>\n",
       "      <td id=\"T_e7d0a_row0_col3\" class=\"data row0 col3\" >0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row1\" class=\"row_heading level0 row1\" >2699</th>\n",
       "      <td id=\"T_e7d0a_row1_col0\" class=\"data row1 col0\" >india</td>\n",
       "      <td id=\"T_e7d0a_row1_col1\" class=\"data row1 col1\" >151</td>\n",
       "      <td id=\"T_e7d0a_row1_col2\" class=\"data row1 col2\" >1428.60</td>\n",
       "      <td id=\"T_e7d0a_row1_col3\" class=\"data row1 col3\" >0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row2\" class=\"row_heading level0 row2\" >2424</th>\n",
       "      <td id=\"T_e7d0a_row2_col0\" class=\"data row2 col0\" >ghana</td>\n",
       "      <td id=\"T_e7d0a_row2_col1\" class=\"data row2 col1\" >4</td>\n",
       "      <td id=\"T_e7d0a_row2_col2\" class=\"data row2 col2\" >34.10</td>\n",
       "      <td id=\"T_e7d0a_row2_col3\" class=\"data row2 col3\" >0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row3\" class=\"row_heading level0 row3\" >5475</th>\n",
       "      <td id=\"T_e7d0a_row3_col0\" class=\"data row3 col0\" >saudi arabia</td>\n",
       "      <td id=\"T_e7d0a_row3_col1\" class=\"data row3 col1\" >5</td>\n",
       "      <td id=\"T_e7d0a_row3_col2\" class=\"data row3 col2\" >36.90</td>\n",
       "      <td id=\"T_e7d0a_row3_col3\" class=\"data row3 col3\" >0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row4\" class=\"row_heading level0 row4\" >7083</th>\n",
       "      <td id=\"T_e7d0a_row4_col0\" class=\"data row4 col0\" >zambia</td>\n",
       "      <td id=\"T_e7d0a_row4_col1\" class=\"data row4 col1\" >3</td>\n",
       "      <td id=\"T_e7d0a_row4_col2\" class=\"data row4 col2\" >20.20</td>\n",
       "      <td id=\"T_e7d0a_row4_col3\" class=\"data row4 col3\" >0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row5\" class=\"row_heading level0 row5\" >4744</th>\n",
       "      <td id=\"T_e7d0a_row5_col0\" class=\"data row5 col0\" >norway</td>\n",
       "      <td id=\"T_e7d0a_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "      <td id=\"T_e7d0a_row5_col2\" class=\"data row5 col2\" >5.50</td>\n",
       "      <td id=\"T_e7d0a_row5_col3\" class=\"data row5 col3\" >0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row6\" class=\"row_heading level0 row6\" >3125</th>\n",
       "      <td id=\"T_e7d0a_row6_col0\" class=\"data row6 col0\" >israel</td>\n",
       "      <td id=\"T_e7d0a_row6_col1\" class=\"data row6 col1\" >2</td>\n",
       "      <td id=\"T_e7d0a_row6_col2\" class=\"data row6 col2\" >9.80</td>\n",
       "      <td id=\"T_e7d0a_row6_col3\" class=\"data row6 col3\" >0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row7\" class=\"row_heading level0 row7\" >2009</th>\n",
       "      <td id=\"T_e7d0a_row7_col0\" class=\"data row7 col0\" >egypt</td>\n",
       "      <td id=\"T_e7d0a_row7_col1\" class=\"data row7 col1\" >32</td>\n",
       "      <td id=\"T_e7d0a_row7_col2\" class=\"data row7 col2\" >105.20</td>\n",
       "      <td id=\"T_e7d0a_row7_col3\" class=\"data row7 col3\" >0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row8\" class=\"row_heading level0 row8\" >3275</th>\n",
       "      <td id=\"T_e7d0a_row8_col0\" class=\"data row8 col0\" >cote d'ivoire</td>\n",
       "      <td id=\"T_e7d0a_row8_col1\" class=\"data row8 col1\" >10</td>\n",
       "      <td id=\"T_e7d0a_row8_col2\" class=\"data row8 col2\" >30.90</td>\n",
       "      <td id=\"T_e7d0a_row8_col3\" class=\"data row8 col3\" >0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7d0a_level0_row9\" class=\"row_heading level0 row9\" >4379</th>\n",
       "      <td id=\"T_e7d0a_row9_col0\" class=\"data row9 col0\" >mozambique</td>\n",
       "      <td id=\"T_e7d0a_row9_col1\" class=\"data row9 col1\" >12</td>\n",
       "      <td id=\"T_e7d0a_row9_col2\" class=\"data row9 col2\" >33.90</td>\n",
       "      <td id=\"T_e7d0a_row9_col3\" class=\"data row9 col3\" >0.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb565c0e860>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by articles_per_capita in ascending order and select the bottom 10\n",
    "bottom_10_countries_by_coverage = country_coverage_df.sort_values(by='articles_per_capita', ascending=True).head(10)\n",
    "\n",
    "# Display the bottom 10 countries by total articles per capita\n",
    "bottom_10_countries_by_coverage = bottom_10_countries_by_coverage.style.format({\n",
    "    'total_articles': '{:.0f}',\n",
    "    'population': '{:.2f}',               # 2 decimal places for population (in millions)\n",
    "    'articles_per_capita': '{:.6f}'       # 6 decimal places for per capita ratios\n",
    "})\n",
    "\n",
    "# Show formatted table\n",
    "bottom_10_countries_by_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8203e1d-84c9-4bf0-b419-6d4dfae12141",
   "metadata": {},
   "source": [
    "This table shows the bottom 10 countries with the lowest number of Wikipedia articles per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ce6c9-c027-407b-822f-bdbea1966bbd",
   "metadata": {},
   "source": [
    "## 3. Top 10 Countries by High-Quality Articles (Highest High-Quality Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4deaf8e9-c4e1-4434-92dc-ade011bf9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high-quality articles: 304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1ca96\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1ca96_level0_col0\" class=\"col_heading level0 col0\" >country</th>\n",
       "      <th id=\"T_1ca96_level0_col1\" class=\"col_heading level0 col1\" >total_high_quality_articles</th>\n",
       "      <th id=\"T_1ca96_level0_col2\" class=\"col_heading level0 col2\" >population</th>\n",
       "      <th id=\"T_1ca96_level0_col3\" class=\"col_heading level0 col3\" >high_quality_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row0\" class=\"row_heading level0 row0\" >4279</th>\n",
       "      <td id=\"T_1ca96_row0_col0\" class=\"data row0 col0\" >montenegro</td>\n",
       "      <td id=\"T_1ca96_row0_col1\" class=\"data row0 col1\" >3</td>\n",
       "      <td id=\"T_1ca96_row0_col2\" class=\"data row0 col2\" >0.60</td>\n",
       "      <td id=\"T_1ca96_row0_col3\" class=\"data row0 col3\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row1\" class=\"row_heading level0 row1\" >3903</th>\n",
       "      <td id=\"T_1ca96_row1_col0\" class=\"data row1 col0\" >luxembourg</td>\n",
       "      <td id=\"T_1ca96_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "      <td id=\"T_1ca96_row1_col2\" class=\"data row1 col2\" >0.70</td>\n",
       "      <td id=\"T_1ca96_row1_col3\" class=\"data row1 col3\" >2.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row2\" class=\"row_heading level0 row2\" >85</th>\n",
       "      <td id=\"T_1ca96_row2_col0\" class=\"data row2 col0\" >albania</td>\n",
       "      <td id=\"T_1ca96_row2_col1\" class=\"data row2 col1\" >7</td>\n",
       "      <td id=\"T_1ca96_row2_col2\" class=\"data row2 col2\" >2.70</td>\n",
       "      <td id=\"T_1ca96_row2_col3\" class=\"data row2 col3\" >2.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row3\" class=\"row_heading level0 row3\" >3667</th>\n",
       "      <td id=\"T_1ca96_row3_col0\" class=\"data row3 col0\" >kosovo</td>\n",
       "      <td id=\"T_1ca96_row3_col1\" class=\"data row3 col1\" >4</td>\n",
       "      <td id=\"T_1ca96_row3_col2\" class=\"data row3 col2\" >1.70</td>\n",
       "      <td id=\"T_1ca96_row3_col3\" class=\"data row3 col3\" >2.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row4\" class=\"row_heading level0 row4\" >4089</th>\n",
       "      <td id=\"T_1ca96_row4_col0\" class=\"data row4 col0\" >maldives</td>\n",
       "      <td id=\"T_1ca96_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "      <td id=\"T_1ca96_row4_col2\" class=\"data row4 col2\" >0.60</td>\n",
       "      <td id=\"T_1ca96_row4_col3\" class=\"data row4 col3\" >1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row5\" class=\"row_heading level0 row5\" >3851</th>\n",
       "      <td id=\"T_1ca96_row5_col0\" class=\"data row5 col0\" >lithuania</td>\n",
       "      <td id=\"T_1ca96_row5_col1\" class=\"data row5 col1\" >4</td>\n",
       "      <td id=\"T_1ca96_row5_col2\" class=\"data row5 col2\" >2.90</td>\n",
       "      <td id=\"T_1ca96_row5_col3\" class=\"data row5 col3\" >1.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row6\" class=\"row_heading level0 row6\" >1726</th>\n",
       "      <td id=\"T_1ca96_row6_col0\" class=\"data row6 col0\" >croatia</td>\n",
       "      <td id=\"T_1ca96_row6_col1\" class=\"data row6 col1\" >5</td>\n",
       "      <td id=\"T_1ca96_row6_col2\" class=\"data row6 col2\" >3.80</td>\n",
       "      <td id=\"T_1ca96_row6_col3\" class=\"data row6 col3\" >1.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row7\" class=\"row_heading level0 row7\" >2551</th>\n",
       "      <td id=\"T_1ca96_row7_col0\" class=\"data row7 col0\" >guyana</td>\n",
       "      <td id=\"T_1ca96_row7_col1\" class=\"data row7 col1\" >1</td>\n",
       "      <td id=\"T_1ca96_row7_col2\" class=\"data row7 col2\" >0.80</td>\n",
       "      <td id=\"T_1ca96_row7_col3\" class=\"data row7 col3\" >1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row8\" class=\"row_heading level0 row8\" >4874</th>\n",
       "      <td id=\"T_1ca96_row8_col0\" class=\"data row8 col0\" >palestinian territory</td>\n",
       "      <td id=\"T_1ca96_row8_col1\" class=\"data row8 col1\" >6</td>\n",
       "      <td id=\"T_1ca96_row8_col2\" class=\"data row8 col2\" >5.50</td>\n",
       "      <td id=\"T_1ca96_row8_col3\" class=\"data row8 col3\" >1.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ca96_level0_row9\" class=\"row_heading level0 row9\" >5663</th>\n",
       "      <td id=\"T_1ca96_row9_col0\" class=\"data row9 col0\" >slovenia</td>\n",
       "      <td id=\"T_1ca96_row9_col1\" class=\"data row9 col1\" >2</td>\n",
       "      <td id=\"T_1ca96_row9_col2\" class=\"data row9 col2\" >2.10</td>\n",
       "      <td id=\"T_1ca96_row9_col3\" class=\"data row9 col3\" >0.952381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb5b40c96c0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for high-quality articles (FA and GA)\n",
    "high_quality_df = final_df[final_df['article_quality'].isin(['FA', 'GA'])].copy()\n",
    "print(f\"Number of high-quality articles: {len(high_quality_df)}\")\n",
    "\n",
    "# Calculate the total number of high-quality articles per country\n",
    "high_quality_df['total_high_quality_articles'] = high_quality_df.groupby('country')['article_title'].transform('count')\n",
    "\n",
    "# Calculate high-quality articles-per-capita\n",
    "high_quality_df['high_quality_per_capita'] = high_quality_df['total_high_quality_articles'] / high_quality_df['population']\n",
    "\n",
    "# Group by country and get the first instance of each country\n",
    "country_high_quality_df = high_quality_df[['country', 'total_high_quality_articles', 'population', 'high_quality_per_capita']].drop_duplicates(subset=['country'])\n",
    "\n",
    "# Sort by high_quality_per_capita and select the top 10\n",
    "top_10_countries_by_high_quality = country_high_quality_df.sort_values(by='high_quality_per_capita', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 countries by high-quality articles per capita\n",
    "top_10_countries_by_high_quality = top_10_countries_by_high_quality.style.format({\n",
    "    'total_high_quality_articles': '{:.0f}',\n",
    "    'population': '{:.2f}',                   # 2 decimal places for population (in millions)\n",
    "    'high_quality_per_capita': '{:.6f}'       # 6 decimal places for per capita ratios\n",
    "})\n",
    "\n",
    "# Show formatted table\n",
    "top_10_countries_by_high_quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d52298-d139-42d4-8306-cd20947cea1c",
   "metadata": {},
   "source": [
    "This table shows the top 10 countries with the highest number of high-quality (FA or GA) Wikipedia articles per capita. Note, here we filtered for countries that have at least one entery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474272e-91bf-411a-bd5d-617b994c08e4",
   "metadata": {},
   "source": [
    "## 4. Bottom 10 Countries by High-Quality Articles (Lowest High-Quality Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c085cf3f-5f28-41ef-9e12-f2db74c101bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_03579\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_03579_level0_col0\" class=\"col_heading level0 col0\" >country</th>\n",
       "      <th id=\"T_03579_level0_col1\" class=\"col_heading level0 col1\" >total_high_quality_articles</th>\n",
       "      <th id=\"T_03579_level0_col2\" class=\"col_heading level0 col2\" >population</th>\n",
       "      <th id=\"T_03579_level0_col3\" class=\"col_heading level0 col3\" >high_quality_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row0\" class=\"row_heading level0 row0\" >636</th>\n",
       "      <td id=\"T_03579_row0_col0\" class=\"data row0 col0\" >bangladesh</td>\n",
       "      <td id=\"T_03579_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_03579_row0_col2\" class=\"data row0 col2\" >173.50</td>\n",
       "      <td id=\"T_03579_row0_col3\" class=\"data row0 col3\" >0.005764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row1\" class=\"row_heading level0 row1\" >2025</th>\n",
       "      <td id=\"T_03579_row1_col0\" class=\"data row1 col0\" >egypt</td>\n",
       "      <td id=\"T_03579_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_03579_row1_col2\" class=\"data row1 col2\" >105.20</td>\n",
       "      <td id=\"T_03579_row1_col3\" class=\"data row1 col3\" >0.009506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row2\" class=\"row_heading level0 row2\" >2101</th>\n",
       "      <td id=\"T_03579_row2_col0\" class=\"data row2 col0\" >ethiopia</td>\n",
       "      <td id=\"T_03579_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "      <td id=\"T_03579_row2_col2\" class=\"data row2 col2\" >126.50</td>\n",
       "      <td id=\"T_03579_row2_col3\" class=\"data row2 col3\" >0.015810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row3\" class=\"row_heading level0 row3\" >3318</th>\n",
       "      <td id=\"T_03579_row3_col0\" class=\"data row3 col0\" >japan</td>\n",
       "      <td id=\"T_03579_row3_col1\" class=\"data row3 col1\" >2</td>\n",
       "      <td id=\"T_03579_row3_col2\" class=\"data row3 col2\" >124.50</td>\n",
       "      <td id=\"T_03579_row3_col3\" class=\"data row3 col3\" >0.016064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row4\" class=\"row_heading level0 row4\" >4775</th>\n",
       "      <td id=\"T_03579_row4_col0\" class=\"data row4 col0\" >pakistan</td>\n",
       "      <td id=\"T_03579_row4_col1\" class=\"data row4 col1\" >4</td>\n",
       "      <td id=\"T_03579_row4_col2\" class=\"data row4 col2\" >240.50</td>\n",
       "      <td id=\"T_03579_row4_col3\" class=\"data row4 col3\" >0.016632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row5\" class=\"row_heading level0 row5\" >1501</th>\n",
       "      <td id=\"T_03579_row5_col0\" class=\"data row5 col0\" >colombia</td>\n",
       "      <td id=\"T_03579_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "      <td id=\"T_03579_row5_col2\" class=\"data row5 col2\" >52.20</td>\n",
       "      <td id=\"T_03579_row5_col3\" class=\"data row5 col3\" >0.019157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row6\" class=\"row_heading level0 row6\" >1617</th>\n",
       "      <td id=\"T_03579_row6_col0\" class=\"data row6 col0\" >congo dr</td>\n",
       "      <td id=\"T_03579_row6_col1\" class=\"data row6 col1\" >2</td>\n",
       "      <td id=\"T_03579_row6_col2\" class=\"data row6 col2\" >102.30</td>\n",
       "      <td id=\"T_03579_row6_col3\" class=\"data row6 col3\" >0.019550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row7\" class=\"row_heading level0 row7\" >7049</th>\n",
       "      <td id=\"T_03579_row7_col0\" class=\"data row7 col0\" >vietnam</td>\n",
       "      <td id=\"T_03579_row7_col1\" class=\"data row7 col1\" >2</td>\n",
       "      <td id=\"T_03579_row7_col2\" class=\"data row7 col2\" >98.90</td>\n",
       "      <td id=\"T_03579_row7_col3\" class=\"data row7 col3\" >0.020222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row8\" class=\"row_heading level0 row8\" >6776</th>\n",
       "      <td id=\"T_03579_row8_col0\" class=\"data row8 col0\" >uganda</td>\n",
       "      <td id=\"T_03579_row8_col1\" class=\"data row8 col1\" >1</td>\n",
       "      <td id=\"T_03579_row8_col2\" class=\"data row8 col2\" >48.60</td>\n",
       "      <td id=\"T_03579_row8_col3\" class=\"data row8 col3\" >0.020576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03579_level0_row9\" class=\"row_heading level0 row9\" >176</th>\n",
       "      <td id=\"T_03579_row9_col0\" class=\"data row9 col0\" >algeria</td>\n",
       "      <td id=\"T_03579_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "      <td id=\"T_03579_row9_col2\" class=\"data row9 col2\" >46.80</td>\n",
       "      <td id=\"T_03579_row9_col3\" class=\"data row9 col3\" >0.021368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb5ac7c90c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by high_quality_per_capita in ascending order and select the bottom 10\n",
    "bottom_10_countries_by_high_quality = country_high_quality_df.sort_values(by='high_quality_per_capita', ascending=True).head(10)\n",
    "\n",
    "# Display the bottom 10 countries by high quality articles per capita\n",
    "bottom_10_countries_by_high_quality = bottom_10_countries_by_high_quality.style.format({\n",
    "    'total_high_quality_articles': '{:.0f}',\n",
    "    'population': '{:.2f}',                   # 2 decimal places for population (in millions)\n",
    "    'high_quality_per_capita': '{:.6f}'       # 6 decimal places for per capita ratios\n",
    "})\n",
    "\n",
    "# Show formatted table\n",
    "bottom_10_countries_by_high_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0ce9b-aa24-442d-b134-7df088a1ac84",
   "metadata": {},
   "source": [
    "This table displays the bottom 10 countries with the lowest number of high-quality Wikipedia articles per capita. Note: I filtered for countries with at leadt one article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750a952-2941-4319-9814-023b8baa39f7",
   "metadata": {},
   "source": [
    "## 5. Geographic Regions by Total Coverage (Ranked by Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a549191-82eb-44e8-b9be-9bb594050989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6c276\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6c276_level0_col0\" class=\"col_heading level0 col0\" >region</th>\n",
       "      <th id=\"T_6c276_level0_col1\" class=\"col_heading level0 col1\" >total_articles</th>\n",
       "      <th id=\"T_6c276_level0_col2\" class=\"col_heading level0 col2\" >Geography</th>\n",
       "      <th id=\"T_6c276_level0_col3\" class=\"col_heading level0 col3\" >Population</th>\n",
       "      <th id=\"T_6c276_level0_col4\" class=\"col_heading level0 col4\" >articles_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row0\" class=\"row_heading level0 row0\" >14</th>\n",
       "      <td id=\"T_6c276_row0_col0\" class=\"data row0 col0\" >SOUTHERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row0_col1\" class=\"data row0 col1\" >797</td>\n",
       "      <td id=\"T_6c276_row0_col2\" class=\"data row0 col2\" >SOUTHERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row0_col3\" class=\"data row0 col3\" >152.00</td>\n",
       "      <td id=\"T_6c276_row0_col4\" class=\"data row0 col4\" >5.243421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "      <td id=\"T_6c276_row1_col0\" class=\"data row1 col0\" >CARIBBEAN</td>\n",
       "      <td id=\"T_6c276_row1_col1\" class=\"data row1 col1\" >219</td>\n",
       "      <td id=\"T_6c276_row1_col2\" class=\"data row1 col2\" >CARIBBEAN</td>\n",
       "      <td id=\"T_6c276_row1_col3\" class=\"data row1 col3\" >44.00</td>\n",
       "      <td id=\"T_6c276_row1_col4\" class=\"data row1 col4\" >4.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row2\" class=\"row_heading level0 row2\" >17</th>\n",
       "      <td id=\"T_6c276_row2_col0\" class=\"data row2 col0\" >WESTERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row2_col1\" class=\"data row2 col1\" >498</td>\n",
       "      <td id=\"T_6c276_row2_col2\" class=\"data row2 col2\" >WESTERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row2_col3\" class=\"data row2 col3\" >199.00</td>\n",
       "      <td id=\"T_6c276_row2_col4\" class=\"data row2 col4\" >2.502513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row3\" class=\"row_heading level0 row3\" >5</th>\n",
       "      <td id=\"T_6c276_row3_col0\" class=\"data row3 col0\" >EASTERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row3_col1\" class=\"data row3 col1\" >709</td>\n",
       "      <td id=\"T_6c276_row3_col2\" class=\"data row3 col2\" >EASTERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row3_col3\" class=\"data row3 col3\" >285.00</td>\n",
       "      <td id=\"T_6c276_row3_col4\" class=\"data row3 col4\" >2.487719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row4\" class=\"row_heading level0 row4\" >16</th>\n",
       "      <td id=\"T_6c276_row4_col0\" class=\"data row4 col0\" >WESTERN ASIA</td>\n",
       "      <td id=\"T_6c276_row4_col1\" class=\"data row4 col1\" >610</td>\n",
       "      <td id=\"T_6c276_row4_col2\" class=\"data row4 col2\" >WESTERN ASIA</td>\n",
       "      <td id=\"T_6c276_row4_col3\" class=\"data row4 col3\" >299.00</td>\n",
       "      <td id=\"T_6c276_row4_col4\" class=\"data row4 col4\" >2.040134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row5\" class=\"row_heading level0 row5\" >8</th>\n",
       "      <td id=\"T_6c276_row5_col0\" class=\"data row5 col0\" >NORTHERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row5_col1\" class=\"data row5 col1\" >191</td>\n",
       "      <td id=\"T_6c276_row5_col2\" class=\"data row5 col2\" >NORTHERN EUROPE</td>\n",
       "      <td id=\"T_6c276_row5_col3\" class=\"data row5 col3\" >108.00</td>\n",
       "      <td id=\"T_6c276_row5_col4\" class=\"data row5 col4\" >1.768519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row6\" class=\"row_heading level0 row6\" >13</th>\n",
       "      <td id=\"T_6c276_row6_col0\" class=\"data row6 col0\" >SOUTHERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row6_col1\" class=\"data row6 col1\" >123</td>\n",
       "      <td id=\"T_6c276_row6_col2\" class=\"data row6 col2\" >SOUTHERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row6_col3\" class=\"data row6 col3\" >70.00</td>\n",
       "      <td id=\"T_6c276_row6_col4\" class=\"data row6 col4\" >1.757143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row7\" class=\"row_heading level0 row7\" >9</th>\n",
       "      <td id=\"T_6c276_row7_col0\" class=\"data row7 col0\" >OCEANIA</td>\n",
       "      <td id=\"T_6c276_row7_col1\" class=\"data row7 col1\" >72</td>\n",
       "      <td id=\"T_6c276_row7_col2\" class=\"data row7 col2\" >OCEANIA</td>\n",
       "      <td id=\"T_6c276_row7_col3\" class=\"data row7 col3\" >45.00</td>\n",
       "      <td id=\"T_6c276_row7_col4\" class=\"data row7 col4\" >1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row8\" class=\"row_heading level0 row8\" >4</th>\n",
       "      <td id=\"T_6c276_row8_col0\" class=\"data row8 col0\" >EASTERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row8_col1\" class=\"data row8 col1\" >665</td>\n",
       "      <td id=\"T_6c276_row8_col2\" class=\"data row8 col2\" >EASTERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row8_col3\" class=\"data row8 col3\" >483.00</td>\n",
       "      <td id=\"T_6c276_row8_col4\" class=\"data row8 col4\" >1.376812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
       "      <td id=\"T_6c276_row9_col0\" class=\"data row9 col0\" >SOUTH AMERICA</td>\n",
       "      <td id=\"T_6c276_row9_col1\" class=\"data row9 col1\" >569</td>\n",
       "      <td id=\"T_6c276_row9_col2\" class=\"data row9 col2\" >SOUTH AMERICA</td>\n",
       "      <td id=\"T_6c276_row9_col3\" class=\"data row9 col3\" >426.00</td>\n",
       "      <td id=\"T_6c276_row9_col4\" class=\"data row9 col4\" >1.335681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row10\" class=\"row_heading level0 row10\" >2</th>\n",
       "      <td id=\"T_6c276_row10_col0\" class=\"data row10 col0\" >CENTRAL ASIA</td>\n",
       "      <td id=\"T_6c276_row10_col1\" class=\"data row10 col1\" >106</td>\n",
       "      <td id=\"T_6c276_row10_col2\" class=\"data row10 col2\" >CENTRAL ASIA</td>\n",
       "      <td id=\"T_6c276_row10_col3\" class=\"data row10 col3\" >80.00</td>\n",
       "      <td id=\"T_6c276_row10_col4\" class=\"data row10 col4\" >1.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row11\" class=\"row_heading level0 row11\" >7</th>\n",
       "      <td id=\"T_6c276_row11_col0\" class=\"data row11 col0\" >NORTHERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row11_col1\" class=\"data row11 col1\" >302</td>\n",
       "      <td id=\"T_6c276_row11_col2\" class=\"data row11 col2\" >NORTHERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row11_col3\" class=\"data row11 col3\" >256.00</td>\n",
       "      <td id=\"T_6c276_row11_col4\" class=\"data row11 col4\" >1.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row12\" class=\"row_heading level0 row12\" >15</th>\n",
       "      <td id=\"T_6c276_row12_col0\" class=\"data row12 col0\" >WESTERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row12_col1\" class=\"data row12 col1\" >515</td>\n",
       "      <td id=\"T_6c276_row12_col2\" class=\"data row12 col2\" >WESTERN AFRICA</td>\n",
       "      <td id=\"T_6c276_row12_col3\" class=\"data row12 col3\" >442.00</td>\n",
       "      <td id=\"T_6c276_row12_col4\" class=\"data row12 col4\" >1.165158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row13\" class=\"row_heading level0 row13\" >6</th>\n",
       "      <td id=\"T_6c276_row13_col0\" class=\"data row13 col0\" >MIDDLE AFRICA</td>\n",
       "      <td id=\"T_6c276_row13_col1\" class=\"data row13 col1\" >231</td>\n",
       "      <td id=\"T_6c276_row13_col2\" class=\"data row13 col2\" >MIDDLE AFRICA</td>\n",
       "      <td id=\"T_6c276_row13_col3\" class=\"data row13 col3\" >202.00</td>\n",
       "      <td id=\"T_6c276_row13_col4\" class=\"data row13 col4\" >1.143564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row14\" class=\"row_heading level0 row14\" >1</th>\n",
       "      <td id=\"T_6c276_row14_col0\" class=\"data row14 col0\" >CENTRAL AMERICA</td>\n",
       "      <td id=\"T_6c276_row14_col1\" class=\"data row14 col1\" >188</td>\n",
       "      <td id=\"T_6c276_row14_col2\" class=\"data row14 col2\" >CENTRAL AMERICA</td>\n",
       "      <td id=\"T_6c276_row14_col3\" class=\"data row14 col3\" >182.00</td>\n",
       "      <td id=\"T_6c276_row14_col4\" class=\"data row14 col4\" >1.032967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row15\" class=\"row_heading level0 row15\" >12</th>\n",
       "      <td id=\"T_6c276_row15_col0\" class=\"data row15 col0\" >SOUTHEAST ASIA</td>\n",
       "      <td id=\"T_6c276_row15_col1\" class=\"data row15 col1\" >396</td>\n",
       "      <td id=\"T_6c276_row15_col2\" class=\"data row15 col2\" >SOUTHEAST ASIA</td>\n",
       "      <td id=\"T_6c276_row15_col3\" class=\"data row15 col3\" >682.00</td>\n",
       "      <td id=\"T_6c276_row15_col4\" class=\"data row15 col4\" >0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row16\" class=\"row_heading level0 row16\" >11</th>\n",
       "      <td id=\"T_6c276_row16_col0\" class=\"data row16 col0\" >SOUTH ASIA</td>\n",
       "      <td id=\"T_6c276_row16_col1\" class=\"data row16 col1\" >670</td>\n",
       "      <td id=\"T_6c276_row16_col2\" class=\"data row16 col2\" >SOUTH ASIA</td>\n",
       "      <td id=\"T_6c276_row16_col3\" class=\"data row16 col3\" >2029.00</td>\n",
       "      <td id=\"T_6c276_row16_col4\" class=\"data row16 col4\" >0.330212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c276_level0_row17\" class=\"row_heading level0 row17\" >3</th>\n",
       "      <td id=\"T_6c276_row17_col0\" class=\"data row17 col0\" >EAST ASIA</td>\n",
       "      <td id=\"T_6c276_row17_col1\" class=\"data row17 col1\" >152</td>\n",
       "      <td id=\"T_6c276_row17_col2\" class=\"data row17 col2\" >EAST ASIA</td>\n",
       "      <td id=\"T_6c276_row17_col3\" class=\"data row17 col3\" >1648.00</td>\n",
       "      <td id=\"T_6c276_row17_col4\" class=\"data row17 col4\" >0.092233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb566a7f5b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will use the population data from regions_df for each region (provided by the DATA 512 teaching team)\n",
    "# regions_df contains the population for each region as per the table provided\n",
    "\n",
    "# Make a copy of regions_df\n",
    "regions_df = regions_df.copy()\n",
    "\n",
    "# Standardize 'Geography' names in regions_df\n",
    "regions_df['Geography'] = regions_df['Geography'].str.strip().str.upper()\n",
    "\n",
    "# Group by region and count the total number of articles per region from the final_df\n",
    "region_coverage_df = final_df.groupby('region').size().reset_index(name='total_articles')\n",
    "\n",
    "# Merge region_coverage_df with regions_df to get the correct population data for each region\n",
    "region_coverage_df = pd.merge(region_coverage_df, regions_df, left_on='region', right_on='Geography')\n",
    "\n",
    "# Calculate total articles-per-capita for each region using the population from regions_df\n",
    "region_coverage_df['articles_per_capita'] = region_coverage_df['total_articles'] / region_coverage_df['Population']\n",
    "\n",
    "# Sort regions by articles-per-capita and display the result\n",
    "regions_by_total_coverage = region_coverage_df.sort_values(by='articles_per_capita', ascending=False)\n",
    "\n",
    "# Display the result: total articles, population, and articles-per-capita for each region\n",
    "regions_by_total_coverage = regions_by_total_coverage.style.format({\n",
    "    'total_articles': '{:.0f}', \n",
    "    'Population': '{:.2f}',               # 2 decimal places for population (in millions)\n",
    "    'articles_per_capita': '{:.6f}'       # 6 decimal places for per capita ratios\n",
    "})\n",
    "\n",
    "# Show formatted table\n",
    "regions_by_total_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e5574-3f9c-4bbd-bcfd-fd38c870bb45",
   "metadata": {},
   "source": [
    "This table shows a rank-ordered list of geographic regions by total articles per capita (highest to lowest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43749ddc-1b89-4aa9-a353-0d7423aa9dae",
   "metadata": {},
   "source": [
    "## 6. Geographic Regions by High-Quality Coverage (Ranked by High-Quality Articles Per Capita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "674f8f33-4c23-41eb-a350-4da3cb2c0e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7b3f2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7b3f2_level0_col0\" class=\"col_heading level0 col0\" >region</th>\n",
       "      <th id=\"T_7b3f2_level0_col1\" class=\"col_heading level0 col1\" >total_high_quality_articles</th>\n",
       "      <th id=\"T_7b3f2_level0_col2\" class=\"col_heading level0 col2\" >Geography</th>\n",
       "      <th id=\"T_7b3f2_level0_col3\" class=\"col_heading level0 col3\" >Population</th>\n",
       "      <th id=\"T_7b3f2_level0_col4\" class=\"col_heading level0 col4\" >high_quality_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row0\" class=\"row_heading level0 row0\" >14</th>\n",
       "      <td id=\"T_7b3f2_row0_col0\" class=\"data row0 col0\" >SOUTHERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row0_col1\" class=\"data row0 col1\" >53</td>\n",
       "      <td id=\"T_7b3f2_row0_col2\" class=\"data row0 col2\" >SOUTHERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row0_col3\" class=\"data row0 col3\" >152.00</td>\n",
       "      <td id=\"T_7b3f2_row0_col4\" class=\"data row0 col4\" >0.348684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "      <td id=\"T_7b3f2_row1_col0\" class=\"data row1 col0\" >CARIBBEAN</td>\n",
       "      <td id=\"T_7b3f2_row1_col1\" class=\"data row1 col1\" >9</td>\n",
       "      <td id=\"T_7b3f2_row1_col2\" class=\"data row1 col2\" >CARIBBEAN</td>\n",
       "      <td id=\"T_7b3f2_row1_col3\" class=\"data row1 col3\" >44.00</td>\n",
       "      <td id=\"T_7b3f2_row1_col4\" class=\"data row1 col4\" >0.204545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "      <td id=\"T_7b3f2_row2_col0\" class=\"data row2 col0\" >EASTERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row2_col1\" class=\"data row2 col1\" >38</td>\n",
       "      <td id=\"T_7b3f2_row2_col2\" class=\"data row2 col2\" >EASTERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row2_col3\" class=\"data row2 col3\" >285.00</td>\n",
       "      <td id=\"T_7b3f2_row2_col4\" class=\"data row2 col4\" >0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row3\" class=\"row_heading level0 row3\" >13</th>\n",
       "      <td id=\"T_7b3f2_row3_col0\" class=\"data row3 col0\" >SOUTHERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row3_col1\" class=\"data row3 col1\" >8</td>\n",
       "      <td id=\"T_7b3f2_row3_col2\" class=\"data row3 col2\" >SOUTHERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row3_col3\" class=\"data row3 col3\" >70.00</td>\n",
       "      <td id=\"T_7b3f2_row3_col4\" class=\"data row3 col4\" >0.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row4\" class=\"row_heading level0 row4\" >17</th>\n",
       "      <td id=\"T_7b3f2_row4_col0\" class=\"data row4 col0\" >WESTERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row4_col1\" class=\"data row4 col1\" >21</td>\n",
       "      <td id=\"T_7b3f2_row4_col2\" class=\"data row4 col2\" >WESTERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row4_col3\" class=\"data row4 col3\" >199.00</td>\n",
       "      <td id=\"T_7b3f2_row4_col4\" class=\"data row4 col4\" >0.105528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row5\" class=\"row_heading level0 row5\" >16</th>\n",
       "      <td id=\"T_7b3f2_row5_col0\" class=\"data row5 col0\" >WESTERN ASIA</td>\n",
       "      <td id=\"T_7b3f2_row5_col1\" class=\"data row5 col1\" >27</td>\n",
       "      <td id=\"T_7b3f2_row5_col2\" class=\"data row5 col2\" >WESTERN ASIA</td>\n",
       "      <td id=\"T_7b3f2_row5_col3\" class=\"data row5 col3\" >299.00</td>\n",
       "      <td id=\"T_7b3f2_row5_col4\" class=\"data row5 col4\" >0.090301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row6\" class=\"row_heading level0 row6\" >8</th>\n",
       "      <td id=\"T_7b3f2_row6_col0\" class=\"data row6 col0\" >NORTHERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row6_col1\" class=\"data row6 col1\" >9</td>\n",
       "      <td id=\"T_7b3f2_row6_col2\" class=\"data row6 col2\" >NORTHERN EUROPE</td>\n",
       "      <td id=\"T_7b3f2_row6_col3\" class=\"data row6 col3\" >108.00</td>\n",
       "      <td id=\"T_7b3f2_row6_col4\" class=\"data row6 col4\" >0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7b3f2_row7_col0\" class=\"data row7 col0\" >NORTHERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row7_col1\" class=\"data row7 col1\" >17</td>\n",
       "      <td id=\"T_7b3f2_row7_col2\" class=\"data row7 col2\" >NORTHERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row7_col3\" class=\"data row7 col3\" >256.00</td>\n",
       "      <td id=\"T_7b3f2_row7_col4\" class=\"data row7 col4\" >0.066406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row8\" class=\"row_heading level0 row8\" >2</th>\n",
       "      <td id=\"T_7b3f2_row8_col0\" class=\"data row8 col0\" >CENTRAL ASIA</td>\n",
       "      <td id=\"T_7b3f2_row8_col1\" class=\"data row8 col1\" >5</td>\n",
       "      <td id=\"T_7b3f2_row8_col2\" class=\"data row8 col2\" >CENTRAL ASIA</td>\n",
       "      <td id=\"T_7b3f2_row8_col3\" class=\"data row8 col3\" >80.00</td>\n",
       "      <td id=\"T_7b3f2_row8_col4\" class=\"data row8 col4\" >0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row9\" class=\"row_heading level0 row9\" >1</th>\n",
       "      <td id=\"T_7b3f2_row9_col0\" class=\"data row9 col0\" >CENTRAL AMERICA</td>\n",
       "      <td id=\"T_7b3f2_row9_col1\" class=\"data row9 col1\" >10</td>\n",
       "      <td id=\"T_7b3f2_row9_col2\" class=\"data row9 col2\" >CENTRAL AMERICA</td>\n",
       "      <td id=\"T_7b3f2_row9_col3\" class=\"data row9 col3\" >182.00</td>\n",
       "      <td id=\"T_7b3f2_row9_col4\" class=\"data row9 col4\" >0.054945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7b3f2_row10_col0\" class=\"data row10 col0\" >SOUTH AMERICA</td>\n",
       "      <td id=\"T_7b3f2_row10_col1\" class=\"data row10 col1\" >19</td>\n",
       "      <td id=\"T_7b3f2_row10_col2\" class=\"data row10 col2\" >SOUTH AMERICA</td>\n",
       "      <td id=\"T_7b3f2_row10_col3\" class=\"data row10 col3\" >426.00</td>\n",
       "      <td id=\"T_7b3f2_row10_col4\" class=\"data row10 col4\" >0.044601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row11\" class=\"row_heading level0 row11\" >6</th>\n",
       "      <td id=\"T_7b3f2_row11_col0\" class=\"data row11 col0\" >MIDDLE AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row11_col1\" class=\"data row11 col1\" >8</td>\n",
       "      <td id=\"T_7b3f2_row11_col2\" class=\"data row11 col2\" >MIDDLE AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row11_col3\" class=\"data row11 col3\" >202.00</td>\n",
       "      <td id=\"T_7b3f2_row11_col4\" class=\"data row11 col4\" >0.039604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_7b3f2_row12_col0\" class=\"data row12 col0\" >SOUTHEAST ASIA</td>\n",
       "      <td id=\"T_7b3f2_row12_col1\" class=\"data row12 col1\" >25</td>\n",
       "      <td id=\"T_7b3f2_row12_col2\" class=\"data row12 col2\" >SOUTHEAST ASIA</td>\n",
       "      <td id=\"T_7b3f2_row12_col3\" class=\"data row12 col3\" >682.00</td>\n",
       "      <td id=\"T_7b3f2_row12_col4\" class=\"data row12 col4\" >0.036657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row13\" class=\"row_heading level0 row13\" >4</th>\n",
       "      <td id=\"T_7b3f2_row13_col0\" class=\"data row13 col0\" >EASTERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row13_col1\" class=\"data row13 col1\" >17</td>\n",
       "      <td id=\"T_7b3f2_row13_col2\" class=\"data row13 col2\" >EASTERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row13_col3\" class=\"data row13 col3\" >483.00</td>\n",
       "      <td id=\"T_7b3f2_row13_col4\" class=\"data row13 col4\" >0.035197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row14\" class=\"row_heading level0 row14\" >15</th>\n",
       "      <td id=\"T_7b3f2_row14_col0\" class=\"data row14 col0\" >WESTERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row14_col1\" class=\"data row14 col1\" >13</td>\n",
       "      <td id=\"T_7b3f2_row14_col2\" class=\"data row14 col2\" >WESTERN AFRICA</td>\n",
       "      <td id=\"T_7b3f2_row14_col3\" class=\"data row14 col3\" >442.00</td>\n",
       "      <td id=\"T_7b3f2_row14_col4\" class=\"data row14 col4\" >0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row15\" class=\"row_heading level0 row15\" >9</th>\n",
       "      <td id=\"T_7b3f2_row15_col0\" class=\"data row15 col0\" >OCEANIA</td>\n",
       "      <td id=\"T_7b3f2_row15_col1\" class=\"data row15 col1\" >1</td>\n",
       "      <td id=\"T_7b3f2_row15_col2\" class=\"data row15 col2\" >OCEANIA</td>\n",
       "      <td id=\"T_7b3f2_row15_col3\" class=\"data row15 col3\" >45.00</td>\n",
       "      <td id=\"T_7b3f2_row15_col4\" class=\"data row15 col4\" >0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row16\" class=\"row_heading level0 row16\" >11</th>\n",
       "      <td id=\"T_7b3f2_row16_col0\" class=\"data row16 col0\" >SOUTH ASIA</td>\n",
       "      <td id=\"T_7b3f2_row16_col1\" class=\"data row16 col1\" >21</td>\n",
       "      <td id=\"T_7b3f2_row16_col2\" class=\"data row16 col2\" >SOUTH ASIA</td>\n",
       "      <td id=\"T_7b3f2_row16_col3\" class=\"data row16 col3\" >2029.00</td>\n",
       "      <td id=\"T_7b3f2_row16_col4\" class=\"data row16 col4\" >0.010350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b3f2_level0_row17\" class=\"row_heading level0 row17\" >3</th>\n",
       "      <td id=\"T_7b3f2_row17_col0\" class=\"data row17 col0\" >EAST ASIA</td>\n",
       "      <td id=\"T_7b3f2_row17_col1\" class=\"data row17 col1\" >3</td>\n",
       "      <td id=\"T_7b3f2_row17_col2\" class=\"data row17 col2\" >EAST ASIA</td>\n",
       "      <td id=\"T_7b3f2_row17_col3\" class=\"data row17 col3\" >1648.00</td>\n",
       "      <td id=\"T_7b3f2_row17_col4\" class=\"data row17 col4\" >0.001820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb565bbfeb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by region and count the total number of high-quality articles per region from high_quality_df\n",
    "region_high_quality_df = high_quality_df.groupby('region').size().reset_index(name='total_high_quality_articles')\n",
    "\n",
    "# Merge region_high_quality_df with regions_df to get the correct population data for each region\n",
    "region_high_quality_df = pd.merge(region_high_quality_df, regions_df, left_on='region', right_on='Geography')\n",
    "\n",
    "# Calculate high-quality articles-per-capita for each region using the population from regions_df\n",
    "region_high_quality_df['high_quality_per_capita'] = region_high_quality_df['total_high_quality_articles'] / region_high_quality_df['Population']\n",
    "\n",
    "# Sort regions by high-quality articles-per-capita and display the result\n",
    "regions_by_high_quality_coverage = region_high_quality_df.sort_values(by='high_quality_per_capita', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "regions_by_high_quality_coverage = regions_by_high_quality_coverage.style.format({\n",
    "    'total_high_quality_articles': '{:.0f}',  # No decimal places for article counts\n",
    "    'Population': '{:.2f}',                   # 2 decimal places for population (in millions)\n",
    "    'high_quality_per_capita': '{:.6f}'       # 6 decimal places for per capita ratios\n",
    "})\n",
    "\n",
    "# Show formatted table\n",
    "regions_by_high_quality_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a1880-e3ef-4941-b6fc-8d793eeab2c0",
   "metadata": {},
   "source": [
    "This table shows a rank-ordered list of geographic regions by high-quality (FA/GA) articles per capita."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
